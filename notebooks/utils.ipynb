{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, confusion_matrix, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_params(metric, label, interp=True):\n",
    "    fpr = []\n",
    "    tpr = []\n",
    "    thr = []\n",
    "    thr_list = list(np.linspace(0, metric.max(),1001))\n",
    "\n",
    "    fp = 1\n",
    "    ind = 0\n",
    "    while fp > 0:\n",
    "        threshold = thr_list[ind]\n",
    "        ind += 1\n",
    "\n",
    "        y = (metric>threshold)\n",
    "        tn, fp, fn, tp = confusion_matrix(label, y).ravel()\n",
    "\n",
    "        fpr.append( fp/(tn + fp) )\n",
    "        tpr.append( tp/(tp + fn) )\n",
    "        thr.append( threshold )\n",
    "\n",
    "    while tp > 0:\n",
    "        threshold = thr_list[ind]\n",
    "        ind += 1\n",
    "        y = (metric>threshold)\n",
    "        tn, fp, fn, tp = confusion_matrix(label, y).ravel()\n",
    "\n",
    "    \n",
    "    fpr = fpr[::-1]\n",
    "    tpr = tpr[::-1]\n",
    "    thr = thr[::-1]\n",
    "\n",
    "    if interp:\n",
    "        fpr_base = np.linspace(0, 1, 101)\n",
    "        tpr = list(np.interp(fpr_base, fpr, tpr))\n",
    "        thr = list(np.interp(fpr_base, fpr, thr))\n",
    "        fpr = list(fpr_base)\n",
    "\n",
    "    fpr.insert(0, 0)\n",
    "    tpr.insert(0, 0)\n",
    "    thr.insert(0, threshold)\n",
    "\n",
    "    return tpr, fpr, thr\n",
    "\n",
    "def compute_auc(tpr, fpr):\n",
    "    auc = 0\n",
    "    for i in range(1, len(fpr)):\n",
    "        auc += (fpr[i] - fpr[i - 1]) * (tpr[i] + tpr[i - 1]) / 2\n",
    "    return auc\n",
    "\n",
    "# def detection(df_metrics, column_name='wse', threshold_min=1000, threshold_max=np.inf, selector='group',\n",
    "#               detection_param='detection_sum', detection_param_threshold=None):\n",
    "#     # df_relevant contains data from nodes that, at some point, have lower<=wse<=upper, and their neighbors.\n",
    "#     # nodes are put into groups if they are close to each other.\n",
    "\n",
    "#     if detection_param_threshold is None:\n",
    "#         detection_param_threshold = df_metrics.timestamp.nunique()//2\n",
    "\n",
    "#     df_relevant = mma.relevant_neighborhood(df_metrics, column_name=column_name,\n",
    "#                                             lower=threshold_min, upper=threshold_max,\n",
    "#                                             only_relevant=True, return_df=True, plot=False, filter_dates=False)\n",
    "\n",
    "#     # Treating disconnected nodes as individual groups. Assining new values\n",
    "#     new_group_values = df_relevant.query('group==0').pid.factorize()[0] + df_relevant.group.max()+1\n",
    "#     df_relevant.loc[df_relevant.group==0, 'group'] = new_group_values\n",
    "\n",
    "\n",
    "#     df_relevant['detection'] = (df_relevant[column_name]>=threshold_min) & (df_relevant[column_name]<=threshold_max)\n",
    "#     df_detection = df_relevant.groupby('pid').agg({column_name:['max','mean'],\n",
    "#                                                     'detection':['sum',mma.consecutive_ones],\n",
    "#                                                     'group':'mean'}).reset_index()\n",
    "\n",
    "#     df_detection.columns = [f\"{level1}_{level2}\" if level2 else level1 for level1, level2 in df_detection.columns]\n",
    "#     df_detection.rename({'group_mean':'group'}, axis=1, inplace=True)\n",
    "\n",
    "#     query = f'{detection_param}>{detection_param_threshold}'\n",
    "#     selected = df_detection.query(query)[selector].unique()\n",
    "\n",
    "#     return df_relevant, selected\n",
    "\n",
    "# def skew(df):\n",
    "#     return np.abs(sp.stats.skew(df.mean_velocity))\n",
    "\n",
    "\n",
    "# def compute_metric(df_test, cut=2, radius=15):\n",
    "\n",
    "#     df_metrics = []\n",
    "#     for cluster in sorted(df_test.cluster.unique()):\n",
    "\n",
    "#         df, nodes = mma.treat_nodes(df_test.query('cluster==@cluster'))\n",
    "#         G, nodes['subgraph'] = mma.NNGraph(nodes, radius=radius, subgraphs=True)\n",
    "\n",
    "#         df_metrics_cluster = []\n",
    "#         for sub_index in sorted(nodes.subgraph.unique())[1:]:\n",
    "\n",
    "#             subnodes = nodes.query('subgraph==@sub_index').copy()\n",
    "#             subdf = df[df.pid.isin(subnodes.pid)].copy()\n",
    "\n",
    "#             G = mma.NNGraph(subnodes, radius=radius)\n",
    "\n",
    "#             w, V = np.linalg.eigh(G.L.toarray())\n",
    "#             wh = np.ones(G.N)\n",
    "#             wh[w<cut] = 0\n",
    "#             Hh = V @ np.diag(wh) @ V.T\n",
    "\n",
    "#             smoothed = subdf[['pid', 'timestamp', 'smoothed' ]].pivot(index='pid', columns='timestamp')\n",
    "\n",
    "#             subdf['hf'] = np.abs((Hh @ smoothed.values).reshape((-1,), order='C'))\n",
    "\n",
    "#             df_metrics_cluster.append(subdf)\n",
    "\n",
    "#         df_metrics_cluster = pd.concat(df_metrics_cluster)\n",
    "#         df_metrics.append(df_metrics_cluster)\n",
    "\n",
    "#     df_metrics = pd.concat(df_metrics)\n",
    "#     return df_metrics\n",
    "\n",
    "\n",
    "# def hfilter(G, cut=2):\n",
    "#     L = G.L.toarray()\n",
    "#     w, V = np.linalg.eigh(L)\n",
    "#     wh = np.ones(G.N)\n",
    "#     wh[w<cut] = 0\n",
    "#     Hh = V @ np.diag(wh) @ V.T\n",
    "#     return Hh\n",
    "\n",
    "# def matplotlib_roc(save=None, ax=None):\n",
    "#     matplotlib.rcParams.update({'font.size': 20})\n",
    "#     matplotlib.rcParams.update({'font.family': 'Times New Roman'})\n",
    "\n",
    "#     if ax is None:\n",
    "#         fig, ax = plt.subplots(figsize=(12,5))\n",
    "#     # sc = ax.scatter(fpr, tpr, c=thr, cmap='viridis', label='Threshold')\n",
    "#     sc = ax.plot(fpr, tpr, linestyle='dotted', linewidth=1, color='black')\n",
    "\n",
    "#     # # Colorbar\n",
    "#     # cbar = plt.colorbar(sc, ax=ax)\n",
    "#     # cbar.set_label('Threshold', rotation=270, labelpad=15)\n",
    "\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     # plt.grid()\n",
    "#     # plt.tight_layout()\n",
    "\n",
    "#     if save is not None:\n",
    "#         plt.savefig(save, transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def synth_graph(seed=0):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Define the length and position of x\n",
    "    lenx = 300\n",
    "    posx = np.arange(0, lenx, 5)\n",
    "\n",
    "    # Define the length and position of y\n",
    "    leny = 3\n",
    "    posy = np.linspace(-15,15,leny)\n",
    "\n",
    "    # Create a meshgrid of x and y\n",
    "    X, Y = np.meshgrid(posx, posy)\n",
    "\n",
    "    # Combine x and y into a single array of positions\n",
    "    pos = np.c_[X.ravel(),Y.ravel()]\n",
    "\n",
    "    # Add random noise to the positions\n",
    "    pos = pos + np.random.randn(pos.shape[0], pos.shape[1])\n",
    "\n",
    "    # Add extra random nodes to the positions\n",
    "    extra_nodes = np.random.uniform(low=[0, -15], high=[lenx, 15], size=(len(pos)//10, 2))\n",
    "    pos = np.r_[pos, extra_nodes]\n",
    "\n",
    "    # Sort the positions by x-coordinate\n",
    "    pos = pos[np.argsort(pos[:,0]),:]\n",
    "\n",
    "    # Create a graph using the positions\n",
    "    G = pygsp.graphs.NNGraph(pos,\n",
    "                            NNtype='radius',\n",
    "                            epsilon = 15, sigma = 100,\n",
    "                            center=False, rescale=False)\n",
    "\n",
    "    plotting_params = {'edge_color':'lightblue', 'edge_width':2,'vertex_color':'black', 'vertex_size':150}\n",
    "    G.plotting.update(plotting_params)\n",
    "    return G\n",
    "\n",
    "def ramp_to_plateou(pos, slope, start=-np.inf, end= np.inf):\n",
    "    mask =  (pos[:,0] >= start) * (pos[:,0]< end)\n",
    "    ramp = slope*(pos[:,0] - start)\n",
    "    ramp = ramp*mask\n",
    "    ramp[pos[:,0]>=end] = ramp.max()\n",
    "    return ramp\n",
    "\n",
    "def create_data(G, anomaly=0.1, size=20, noise_var = 1e-2, signal_power = 1, seed=None, max_slope=1, eigs=1):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    pos = G.coords\n",
    "\n",
    "    w, V = np.linalg.eigh(G.L.toarray())\n",
    "    w[eigs:] = 0 # Frequency filter\n",
    "\n",
    "    displacement = np.random.randn(G.N, size)\n",
    "    displacement = V @ np.diag(w) @ V.T @ displacement # filtering\n",
    "    # normalizing for desired power\n",
    "    displacement = np.sqrt(signal_power*G.N)*displacement/(np.linalg.norm(displacement,axis=0))\n",
    "\n",
    "    noise = np.sqrt(noise_var)*np.random.randn(G.N,size)\n",
    "\n",
    "    # terrain corresponds to a ramp to a plateou in the horizontal direction\n",
    "    slope = max_slope*np.random.rand() # makes small difference given proportional anomaly and scaler\n",
    "    start = pos[:,0].max()*np.random.rand()*0.5 # Slope always start in the first half\n",
    "    # end = start + (pos[:,0].max()-start)*np.random.rand()\n",
    "\n",
    "    min_slope_dist = 100\n",
    "    end = start + min_slope_dist + (pos[:,0].max()-start-min_slope_dist)*np.random.rand() #At least 50m of slope\n",
    "\n",
    "    terrain = ramp_to_plateou(pos, slope=slope, start=start, end=end).reshape((-1,1))\n",
    "    terrain = np.tile(terrain, (1,size)) # Matching number of samples (size)\n",
    "\n",
    "    ptp = terrain.ptp()\n",
    "\n",
    "    signal = displacement + noise + terrain\n",
    "    label = np.zeros(signal.shape)\n",
    "\n",
    "    for timestamp in range(size):\n",
    "\n",
    "        anomalous_sensors = np.vstack([\n",
    "                             np.random.choice(np.arange(0, G.N//3), size=(2,1), replace=False),\n",
    "                             np.random.choice(np.arange(G.N//3, 2*G.N//3), size=(2,1), replace=False),\n",
    "                             np.random.choice(np.arange(2*G.N//3, G.N), size=(2,1), replace=False)]\n",
    "                            ).flatten()\n",
    "        signal[anomalous_sensors, timestamp] += np.random.choice([anomaly*ptp, -anomaly*ptp], size=(6,))\n",
    "        label[anomalous_sensors, timestamp] = 1\n",
    "\n",
    "    return signal, label"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
