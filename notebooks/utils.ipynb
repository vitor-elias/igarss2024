{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, confusion_matrix, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_params(metric, label, interp=True):\n",
    "    fpr = []\n",
    "    tpr = []\n",
    "    thr = []\n",
    "    thr_list = list(np.linspace(0, metric.max(),1001))\n",
    "\n",
    "    fp = 1\n",
    "    ind = 0\n",
    "    while fp > 0:\n",
    "        threshold = thr_list[ind]\n",
    "        ind += 1\n",
    "\n",
    "        y = (metric>threshold)\n",
    "        tn, fp, fn, tp = confusion_matrix(label, y).ravel()\n",
    "\n",
    "        fpr.append( fp/(tn + fp) )\n",
    "        tpr.append( tp/(tp + fn) )\n",
    "        thr.append( threshold )\n",
    "\n",
    "    while tp > 0:\n",
    "        threshold = thr_list[ind]\n",
    "        ind += 1\n",
    "        y = (metric>threshold)\n",
    "        tn, fp, fn, tp = confusion_matrix(label, y).ravel()\n",
    "\n",
    "    \n",
    "    fpr = fpr[::-1]\n",
    "    tpr = tpr[::-1]\n",
    "    thr = thr[::-1]\n",
    "\n",
    "    if interp:\n",
    "        fpr_base = np.linspace(0, 1, 101)\n",
    "        tpr = list(np.interp(fpr_base, fpr, tpr))\n",
    "        thr = list(np.interp(fpr_base, fpr, thr))\n",
    "        fpr = list(fpr_base)\n",
    "\n",
    "    fpr.insert(0, 0)\n",
    "    tpr.insert(0, 0)\n",
    "    thr.insert(0, threshold)\n",
    "\n",
    "    return tpr, fpr, thr\n",
    "\n",
    "def compute_auc(tpr, fpr):\n",
    "    auc = 0\n",
    "    for i in range(1, len(fpr)):\n",
    "        auc += (fpr[i] - fpr[i - 1]) * (tpr[i] + tpr[i - 1]) / 2\n",
    "    return auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic graph generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_graph(N, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    N_grid_points = int(np.floor(0.9*N))\n",
    "    N_rand_points = int(np.ceil(0.1*N))\n",
    "\n",
    "    connected = False\n",
    "\n",
    "    while not connected:\n",
    "\n",
    "        # Generating grid points\n",
    "\n",
    "        # Initialize starting position\n",
    "        current_position = [0, 0]\n",
    "\n",
    "        # Store visited points\n",
    "        visited_points = set()\n",
    "        visited_points.add(tuple(current_position))\n",
    "\n",
    "        # Generate N points\n",
    "        while len(visited_points) < N_grid_points:\n",
    "            # Update the current position by taking a vertical or horizontal step\n",
    "            # horizontal can be +-5, vertical can be +-15\n",
    "\n",
    "            if np.random.rand()>0.5:\n",
    "                current_position[0] += np.random.choice([-5, 5])\n",
    "            else:\n",
    "                current_position[1] += np.random.choice([-14, 14])\n",
    "\n",
    "            # Add the new position to the set of visited points\n",
    "            visited_points.add(tuple(current_position))\n",
    "\n",
    "        points_list = list(visited_points)\n",
    "\n",
    "        # Generating random points\n",
    "        reference_pos = np.random.choice(a=N_grid_points, size=N_rand_points, replace=False)\n",
    "        reference_points = [list(points_list[i]) for i in reference_pos]\n",
    "\n",
    "        for point in reference_points:\n",
    "            point[0] += 10*np.random.random()\n",
    "            point[1] += 10*np.random.random()\n",
    "            visited_points.add(tuple(point))\n",
    "        \n",
    "\n",
    "        # Convert the set of visited points to a list\n",
    "        pos = np.array(list(visited_points))\n",
    "        pos = pos + 0.5*np.random.randn(pos.shape[0], pos.shape[1])\n",
    "\n",
    "        radius=15\n",
    "        sigma=radius**2\n",
    "\n",
    "        G = pygsp.graphs.NNGraph(pos,\n",
    "                                NNtype='radius',\n",
    "                                epsilon = radius, sigma = sigma,\n",
    "                                center=False, rescale=False)\n",
    "        connected = G.is_connected()\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ramp(G, size):\n",
    "    # Auxiliary function for generating data.\n",
    "    # Returns array with \"size\" random ramps on the given graph. A ramp emulates regions with different behaviors.\n",
    "    # Direction of each ramp is randomly horizontal or vertical\n",
    "    # Slope, start and end positions of the ramp are random, within some constraints.\n",
    "    \n",
    "    pos = G.coords\n",
    "    min_slope = 0.1\n",
    "    max_slope = 1\n",
    "    \n",
    "    # Initialize an empty matrix to store ramp vectors\n",
    "    ramp_matrix = np.zeros((len(pos), size))\n",
    "\n",
    "    for i in range(size):\n",
    "        # Randomly choose the direction of the ramp (0, horizontal or 1, vertical)\n",
    "        if np.random.rand() > 0.5:\n",
    "            direction = 0\n",
    "        else:\n",
    "            direction = 1\n",
    "\n",
    "        # Generate a random slope within the specified maximum slope\n",
    "        slope = np.random.uniform(low=min_slope, high=max_slope)\n",
    "\n",
    "        # Calculate the minimum ramp length as a fraction of the peak-to-peak range\n",
    "        min_slope_dist = 0.25 * pos[:, direction].ptp()\n",
    "\n",
    "        # Determine the starting and ending positions\n",
    "        start = pos[:, direction].min() + pos[:, direction].ptp() * np.random.rand() * 0.5\n",
    "        end = start + min_slope_dist + (pos[:, direction].max() - start - min_slope_dist) * np.random.rand()\n",
    "\n",
    "        # Generate the ramp vector based on the slope and the position within the range\n",
    "        ramp = slope * (pos[:, direction] - start)\n",
    "\n",
    "        # Applying a mask that defines where the ramp exists\n",
    "        mask = (pos[:, direction] >= start) * (pos[:, direction] < end)\n",
    "        ramp = ramp * mask\n",
    "\n",
    "        # Set values beyond the end of the ramp to the maximum ramp value\n",
    "        ramp[pos[:, direction] >= end] = ramp.max()\n",
    "\n",
    "        # Store the ramp vector in the matrix\n",
    "        ramp_matrix[:, i] = ramp\n",
    "\n",
    "    return ramp_matrix\n",
    "\n",
    "def generate_smooth(G, size):\n",
    "    # Auxiliary function for generating data\n",
    "    # Returns array with \"size\" smooth signals by filtering white noise with the two first eigenvectors of the graph\n",
    "    \n",
    "    w, V = np.linalg.eigh(G.L.toarray())\n",
    "\n",
    "    # Low-pass filter\n",
    "    h = np.ones(len(w))\n",
    "    h[0] = 1\n",
    "    h[1] = 0.1\n",
    "    h[2:] = 0 \n",
    "\n",
    "    # Generating and filtering white noise to create a smooth graph signal\n",
    "    displacement = np.random.randn(G.N, size) \n",
    "    displacement = V @ np.diag(h) @ V.T @ displacement\n",
    "\n",
    "    # Normalizing signal: average power sum(x^2)/N = 1\n",
    "    displacement = np.sqrt(G.N)*displacement/(np.linalg.norm(displacement,axis=0))\n",
    "\n",
    "    return displacement\n",
    "\n",
    "\n",
    "def pick_non_adjacent(G, n):\n",
    "    # Auxiliary function for generating anomalies.\n",
    "    # Picks n nodes that are not adjacent to each other.\n",
    "    A = G.A.toarray()\n",
    "    np.fill_diagonal(A,1)\n",
    "\n",
    "    tries = 0\n",
    "\n",
    "    while tries < 100000:\n",
    "        tries+=1\n",
    "\n",
    "        fail = 0\n",
    "        possible_nodes= list(np.arange(G.N))\n",
    "        picked_nodes = []\n",
    "\n",
    "        for node in range(n):\n",
    "            if len(possible_nodes) == 0:\n",
    "                fail = 1\n",
    "            else:\n",
    "                pick = np.random.choice(possible_nodes)\n",
    "                picked_nodes.append(pick)\n",
    "                neighbors = np.where(A[pick,:])[0]\n",
    "                possible_nodes = list( set(possible_nodes) - set(neighbors)  )\n",
    "\n",
    "        if not fail:\n",
    "            return picked_nodes\n",
    "    \n",
    "    print('Maximum tries reached. Reduce n')\n",
    "\n",
    "def generate_anomaly(G, signal, anomaly_type=1):\n",
    "    # signal is expected to be a matrix containing different signals in each column\n",
    "    # each column will receive different anomalies\n",
    "\n",
    "    if anomaly_type==1:\n",
    "        anomaly_factor_min = 0.05\n",
    "        anomaly_factor_max = 0.15\n",
    "    elif anomaly_type==2:\n",
    "        anomaly_factor_min = 0.10\n",
    "        anomaly_factor_max = 0.20\n",
    "\n",
    "    N = signal.shape[0]\n",
    "    size = signal.shape[1]\n",
    "\n",
    "    # Defining number of anomalous sensors per signal. Each column of signal has a different number of anomalies\n",
    "    # This value is up to 5% the number of sensors (rounded up)\n",
    "    percentage_per_signal = 0.05*np.random.rand(size) + 1e-5 # value added to avoid zero\n",
    "    number_per_signal = np.ceil(percentage_per_signal*N)\n",
    "\n",
    "    anomalous_positions = [pick_non_adjacent(G,int(n)) for n in number_per_signal]\n",
    "\n",
    "    anomaly = np.zeros(signal.shape)\n",
    "    for i in range(size):\n",
    "        n = int(number_per_signal[i])\n",
    "        anomalous_positions = pick_non_adjacent(G,n)\n",
    "\n",
    "        anomaly[anomalous_positions, i] = signal[:,i].ptp()*np.random.uniform(low=anomaly_factor_min,\n",
    "                                                                              high=anomaly_factor_max,\n",
    "                                                                              size=n)*np.random.choice([+1,-1],size=n)\n",
    "        \n",
    "    label = np.zeros(signal.shape)\n",
    "    label[np.where(anomaly)] = 1\n",
    "\n",
    "    return anomaly, label\n",
    "\n",
    "def generate_data(G, size, anomaly_type=1):    \n",
    "\n",
    "    # Generating healthy data\n",
    "    displacement = generate_smooth(G, size)\n",
    "    ramp = generate_ramp(G, size)\n",
    "    noise = np.sqrt(0.1)*np.random.randn(G.N,size) #noise power = 0.1, SNR = 20 dB\n",
    "\n",
    "    signal = displacement + ramp + noise\n",
    "\n",
    "    # Introducing anomalies\n",
    "    anomaly, label = generate_anomaly(G, signal, anomaly_type)\n",
    "    signal = signal + anomaly\n",
    "    \n",
    "    return signal, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted spectral energy utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wse(nodes, x, radius=15, decay=5, window_norm='energy', cut=2):\n",
    "    S, w, V = WGFT(nodes, x, radius, decay, window_norm)\n",
    "    PSD = S**2\n",
    "    w[w<cut] = 0\n",
    "    return np.sum(PSD*w.reshape(-1,1), axis=0) \n",
    "\n",
    "def WGFT(nodes, x, radius=15, decay=5, window_norm='energy'):\n",
    "    # nodes is a dataframe with easting and northing coordinates\n",
    "    # x is an array of dim = (number_of_nodes, number_of_samples), with at least 1 column\n",
    "\n",
    "    # Applies window hm to signal x, for all nodes, to obtain the power spectral density S\n",
    "\n",
    "    # x = x/np.linalg.norm(x)\n",
    "    # x = x - x.mean(axis=0)\n",
    "     \n",
    "    G = NNGraph(nodes, radius=radius)\n",
    "    w, V = np.linalg.eigh(G.L.toarray())\n",
    "\n",
    "    # creating shapes for single matrix processing of several samples at once\n",
    "    nsamples = x.shape[1] # samples of an entire graph signal. Probably the number of timestamps\n",
    "    X = np.tile(x, reps=(1, G.N)) # For each node, its samples. N times\n",
    "\n",
    "    # Creating window\n",
    "    Hm = spectral_window(w, V, decay, L=G.L.toarray(), norm=window_norm)\n",
    "    Hm = np.repeat(Hm,repeats=nsamples, axis=1) # For each node, its window. nsamples times.\n",
    "\n",
    "    HmX = Hm*X\n",
    "\n",
    "    # Power spectral density S\n",
    "    S = V.T.dot(HmX)\n",
    "    return S, w, V\n",
    "\n",
    "def spectral_window(w, V, decay, L, norm='energy' ):\n",
    "\n",
    "    h_hat = np.exp(-w*decay) # creating spectral window\n",
    "    H_hat = h_hat.reshape((-1,1)).repeat(len(w), axis=1) # repeating the window N times to create shifted versions\n",
    "    Hm_hat = H_hat*(V.T) # creating spectral shifted versions by hadamard product with the m-th row of V\n",
    "    Hm = V.dot(Hm_hat) # obtaining windows in the vertex domain with the IGFT\n",
    "\n",
    "    # Original normalization by norm preserves energy of the windowing\n",
    "    if norm == 'energy':\n",
    "        Hm = Hm/np.linalg.norm(Hm,axis=0)\n",
    "\n",
    "\n",
    "    # Normalization to unitary smoothness, makes different nodes have the same baseline.\n",
    "    if norm == 'smoothness':\n",
    "        # w[w<0] = 0\n",
    "        # L = V.dot(np.diag(w).dot(V.T))\n",
    "        # Hm = Hm/np.sqrt(( (Hm_hat**2).T.dot(w) )).reshape((1,-1))\n",
    "        smoothness = np.diag(Hm.T.dot(L.dot(Hm)))\n",
    "        Hm = Hm/np.sqrt(smoothness).reshape((1,-1)) # sqrt(hTLh)\n",
    "\n",
    "    return Hm\n",
    "\n",
    "def NNGraph(nodes, radius, plotting_params = None, subgraphs=False):\n",
    "    # Nodes are connected if within <radius> m of each other. This can yield disconnected subgraphs\n",
    "    # if \"subgraphs\" is True, we also identify those subgraphs.\n",
    "\n",
    "    sigma = radius**2  #w = np.exp(-(dist**2)/sigma) > w = 0.36 at max radius\n",
    "    G = pygsp.graphs.NNGraph(nodes[['easting','northing']].values,\n",
    "                            NNtype='radius',\n",
    "                            epsilon = radius, sigma = sigma,\n",
    "                            center=False, rescale=False)\n",
    "\n",
    "    # Plotting\n",
    "    if plotting_params is None:\n",
    "        plotting_params = {'edge_color':'darkgray', 'edge_width':1.5,'vertex_color':'black', 'vertex_size':150}\n",
    "    G.plotting.update(plotting_params)\n",
    "\n",
    "    if subgraphs:\n",
    "        subgraph_labels = sp.sparse.csgraph.connected_components(G.A.toarray(), directed=False)[1]\n",
    "        subgraph_labels[G.d==0] = -1\n",
    "        return G, pd.factorize(subgraph_labels, sort=True)[0]\n",
    "    else:\n",
    "        return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph filter utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hfilter(G, cut=2):\n",
    "    L = G.L.toarray()\n",
    "    w, V = np.linalg.eigh(L)\n",
    "    wh = np.ones(G.N)\n",
    "    wh[w<cut] = 0\n",
    "    Hh = V @ np.diag(wh) @ V.T\n",
    "    return Hh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igarss-REga8jWa-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
