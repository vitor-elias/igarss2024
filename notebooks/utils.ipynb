{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, confusion_matrix, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_params(metric, label, interp=True):\n",
    "    fpr = []\n",
    "    tpr = []\n",
    "    thr = []\n",
    "    thr_list = list(np.linspace(0, metric.max(),1001))\n",
    "\n",
    "    fp = 1\n",
    "    ind = 0\n",
    "    while fp > 0:\n",
    "        threshold = thr_list[ind]\n",
    "        ind += 1\n",
    "\n",
    "        y = (metric>threshold)\n",
    "        tn, fp, fn, tp = confusion_matrix(label, y).ravel()\n",
    "\n",
    "        fpr.append( fp/(tn + fp) )\n",
    "        tpr.append( tp/(tp + fn) )\n",
    "        thr.append( threshold )\n",
    "\n",
    "    while tp > 0:\n",
    "        threshold = thr_list[ind]\n",
    "        ind += 1\n",
    "        y = (metric>threshold)\n",
    "        tn, fp, fn, tp = confusion_matrix(label, y).ravel()\n",
    "\n",
    "    \n",
    "    fpr = fpr[::-1]\n",
    "    tpr = tpr[::-1]\n",
    "    thr = thr[::-1]\n",
    "\n",
    "    if interp:\n",
    "        fpr_base = np.linspace(0, 1, 101)\n",
    "        tpr = list(np.interp(fpr_base, fpr, tpr))\n",
    "        thr = list(np.interp(fpr_base, fpr, thr))\n",
    "        fpr = list(fpr_base)\n",
    "\n",
    "    fpr.insert(0, 0)\n",
    "    tpr.insert(0, 0)\n",
    "    thr.insert(0, threshold)\n",
    "\n",
    "    return tpr, fpr, thr\n",
    "\n",
    "def compute_auc(tpr, fpr):\n",
    "    auc = 0\n",
    "    for i in range(1, len(fpr)):\n",
    "        auc += (fpr[i] - fpr[i - 1]) * (tpr[i] + tpr[i - 1]) / 2\n",
    "    return auc\n",
    "\n",
    "# def detection(df_metrics, column_name='wse', threshold_min=1000, threshold_max=np.inf, selector='group',\n",
    "#               detection_param='detection_sum', detection_param_threshold=None):\n",
    "#     # df_relevant contains data from nodes that, at some point, have lower<=wse<=upper, and their neighbors.\n",
    "#     # nodes are put into groups if they are close to each other.\n",
    "\n",
    "#     if detection_param_threshold is None:\n",
    "#         detection_param_threshold = df_metrics.timestamp.nunique()//2\n",
    "\n",
    "#     df_relevant = mma.relevant_neighborhood(df_metrics, column_name=column_name,\n",
    "#                                             lower=threshold_min, upper=threshold_max,\n",
    "#                                             only_relevant=True, return_df=True, plot=False, filter_dates=False)\n",
    "\n",
    "#     # Treating disconnected nodes as individual groups. Assining new values\n",
    "#     new_group_values = df_relevant.query('group==0').pid.factorize()[0] + df_relevant.group.max()+1\n",
    "#     df_relevant.loc[df_relevant.group==0, 'group'] = new_group_values\n",
    "\n",
    "\n",
    "#     df_relevant['detection'] = (df_relevant[column_name]>=threshold_min) & (df_relevant[column_name]<=threshold_max)\n",
    "#     df_detection = df_relevant.groupby('pid').agg({column_name:['max','mean'],\n",
    "#                                                     'detection':['sum',mma.consecutive_ones],\n",
    "#                                                     'group':'mean'}).reset_index()\n",
    "\n",
    "#     df_detection.columns = [f\"{level1}_{level2}\" if level2 else level1 for level1, level2 in df_detection.columns]\n",
    "#     df_detection.rename({'group_mean':'group'}, axis=1, inplace=True)\n",
    "\n",
    "#     query = f'{detection_param}>{detection_param_threshold}'\n",
    "#     selected = df_detection.query(query)[selector].unique()\n",
    "\n",
    "#     return df_relevant, selected\n",
    "\n",
    "# def skew(df):\n",
    "#     return np.abs(sp.stats.skew(df.mean_velocity))\n",
    "\n",
    "\n",
    "# def compute_metric(df_test, cut=2, radius=15):\n",
    "\n",
    "#     df_metrics = []\n",
    "#     for cluster in sorted(df_test.cluster.unique()):\n",
    "\n",
    "#         df, nodes = mma.treat_nodes(df_test.query('cluster==@cluster'))\n",
    "#         G, nodes['subgraph'] = mma.NNGraph(nodes, radius=radius, subgraphs=True)\n",
    "\n",
    "#         df_metrics_cluster = []\n",
    "#         for sub_index in sorted(nodes.subgraph.unique())[1:]:\n",
    "\n",
    "#             subnodes = nodes.query('subgraph==@sub_index').copy()\n",
    "#             subdf = df[df.pid.isin(subnodes.pid)].copy()\n",
    "\n",
    "#             G = mma.NNGraph(subnodes, radius=radius)\n",
    "\n",
    "#             w, V = np.linalg.eigh(G.L.toarray())\n",
    "#             wh = np.ones(G.N)\n",
    "#             wh[w<cut] = 0\n",
    "#             Hh = V @ np.diag(wh) @ V.T\n",
    "\n",
    "#             smoothed = subdf[['pid', 'timestamp', 'smoothed' ]].pivot(index='pid', columns='timestamp')\n",
    "\n",
    "#             subdf['hf'] = np.abs((Hh @ smoothed.values).reshape((-1,), order='C'))\n",
    "\n",
    "#             df_metrics_cluster.append(subdf)\n",
    "\n",
    "#         df_metrics_cluster = pd.concat(df_metrics_cluster)\n",
    "#         df_metrics.append(df_metrics_cluster)\n",
    "\n",
    "#     df_metrics = pd.concat(df_metrics)\n",
    "#     return df_metrics\n",
    "\n",
    "\n",
    "# def hfilter(G, cut=2):\n",
    "#     L = G.L.toarray()\n",
    "#     w, V = np.linalg.eigh(L)\n",
    "#     wh = np.ones(G.N)\n",
    "#     wh[w<cut] = 0\n",
    "#     Hh = V @ np.diag(wh) @ V.T\n",
    "#     return Hh\n",
    "\n",
    "# def matplotlib_roc(save=None, ax=None):\n",
    "#     matplotlib.rcParams.update({'font.size': 20})\n",
    "#     matplotlib.rcParams.update({'font.family': 'Times New Roman'})\n",
    "\n",
    "#     if ax is None:\n",
    "#         fig, ax = plt.subplots(figsize=(12,5))\n",
    "#     # sc = ax.scatter(fpr, tpr, c=thr, cmap='viridis', label='Threshold')\n",
    "#     sc = ax.plot(fpr, tpr, linestyle='dotted', linewidth=1, color='black')\n",
    "\n",
    "#     # # Colorbar\n",
    "#     # cbar = plt.colorbar(sc, ax=ax)\n",
    "#     # cbar.set_label('Threshold', rotation=270, labelpad=15)\n",
    "\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     # plt.grid()\n",
    "#     # plt.tight_layout()\n",
    "\n",
    "#     if save is not None:\n",
    "#         plt.savefig(save, transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def synth_graph(seed=0):\n",
    "#     if seed is not None:\n",
    "#         np.random.seed(seed)\n",
    "\n",
    "#     # Define the length and position of x\n",
    "#     minx = 0\n",
    "#     maxx = 300\n",
    "#     stepx = 5\n",
    "#     posx = np.arange(minx, maxx, stepx)\n",
    "\n",
    "#     # Define the length and position of y\n",
    "#     miny = -15\n",
    "#     maxy = +15\n",
    "#     numy = 3\n",
    "#     posy = np.linspace(miny, maxy, numy)\n",
    "\n",
    "#     # Create a meshgrid of x and y\n",
    "#     X, Y = np.meshgrid(posx, posy)\n",
    "\n",
    "#     # Combine x and y into a single array of positions\n",
    "#     pos = np.c_[X.ravel(),Y.ravel()]\n",
    "\n",
    "#     # Add random noise to the positions\n",
    "#     pos = pos + np.random.randn(pos.shape[0], pos.shape[1])\n",
    "\n",
    "#     # Add extra random nodes to the positions\n",
    "#     extra_nodes = np.random.uniform(low=[0, -15], high=[maxx, 15], size=(len(pos)//10, 2))\n",
    "#     pos = np.r_[pos, extra_nodes]\n",
    "\n",
    "#     # Sort the positions by x-coordinate\n",
    "#     pos = pos[np.argsort(pos[:,0]),:]\n",
    "\n",
    "#     # Create a graph using the positions\n",
    "#     G = pygsp.graphs.NNGraph(pos,\n",
    "#                             NNtype='radius',\n",
    "#                             epsilon = 15, sigma = 100,\n",
    "#                             center=False, rescale=False)\n",
    "\n",
    "#     plotting_params = {'edge_color':'lightblue', 'edge_width':2,'vertex_color':'black', 'vertex_size':150}\n",
    "#     G.plotting.update(plotting_params)\n",
    "#     return G\n",
    "\n",
    "# def ramp_to_plateou(pos, slope, start=-np.inf, end= np.inf):\n",
    "#     mask =  (pos[:,0] >= start) * (pos[:,0]< end)\n",
    "#     ramp = slope*(pos[:,0] - start)\n",
    "#     ramp = ramp*mask\n",
    "#     ramp[pos[:,0]>=end] = ramp.max()\n",
    "#     return ramp\n",
    "\n",
    "# def create_data(G, anomaly=0.1, size=20, noise_var = 1e-2, signal_power = 1, seed=None, max_slope=1, eigs=1):\n",
    "\n",
    "#     np.random.seed(seed)\n",
    "\n",
    "#     pos = G.coords\n",
    "\n",
    "#     w, V = np.linalg.eigh(G.L.toarray())\n",
    "#     w[eigs:] = 0 # Frequency filter\n",
    "\n",
    "#     displacement = np.random.randn(G.N, size)\n",
    "#     displacement = V @ np.diag(w) @ V.T @ displacement # filtering\n",
    "#     # normalizing for desired power\n",
    "#     displacement = np.sqrt(signal_power*G.N)*displacement/(np.linalg.norm(displacement,axis=0))\n",
    "\n",
    "#     noise = np.sqrt(noise_var)*np.random.randn(G.N,size)\n",
    "\n",
    "#     # terrain corresponds to a ramp to a plateou in the horizontal direction\n",
    "#     slope = max_slope*np.random.rand() # makes small difference given proportional anomaly and scaler\n",
    "#     start = pos[:,0].max()*np.random.rand()*0.5 # Slope always start in the first half\n",
    "#     # end = start + (pos[:,0].max()-start)*np.random.rand()\n",
    "\n",
    "#     min_slope_dist = 100\n",
    "#     end = start + min_slope_dist + (pos[:,0].max()-start-min_slope_dist)*np.random.rand() #At least 50m of slope\n",
    "\n",
    "#     terrain = ramp_to_plateou(pos, slope=slope, start=start, end=end).reshape((-1,1))\n",
    "#     terrain = np.tile(terrain, (1,size)) # Matching number of samples (size)\n",
    "\n",
    "#     ptp = terrain.ptp()\n",
    "\n",
    "#     signal = displacement + noise + terrain\n",
    "#     label = np.zeros(signal.shape)\n",
    "\n",
    "#     for timestamp in range(size):\n",
    "\n",
    "#         anomalous_sensors = np.vstack([\n",
    "#                              np.random.choice(np.arange(0, G.N//3), size=(2,1), replace=False),\n",
    "#                              np.random.choice(np.arange(G.N//3, 2*G.N//3), size=(2,1), replace=False),\n",
    "#                              np.random.choice(np.arange(2*G.N//3, G.N), size=(2,1), replace=False)]\n",
    "#                             ).flatten()\n",
    "#         signal[anomalous_sensors, timestamp] += np.random.choice([anomaly*ptp, -anomaly*ptp], size=(6,))\n",
    "#         label[anomalous_sensors, timestamp] = 1\n",
    "\n",
    "#     return signal, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_graph(N, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    N_grid_points = int(np.floor(0.9*N))\n",
    "    N_rand_points = int(np.ceil(0.1*N))\n",
    "\n",
    "    connected = False\n",
    "\n",
    "    while not connected:\n",
    "\n",
    "        # Generating grid points\n",
    "\n",
    "        # Initialize starting position\n",
    "        current_position = [0, 0]\n",
    "\n",
    "        # Store visited points\n",
    "        visited_points = set()\n",
    "        visited_points.add(tuple(current_position))\n",
    "\n",
    "        # Generate N points\n",
    "        while len(visited_points) < N_grid_points:\n",
    "            # Update the current position by taking a vertical or horizontal step\n",
    "            # horizontal can be +-5, vertical can be +-15\n",
    "\n",
    "            if np.random.rand()>0.5:\n",
    "                current_position[0] += np.random.choice([-5, 5])\n",
    "            else:\n",
    "                current_position[1] += np.random.choice([-14, 14])\n",
    "\n",
    "            # Add the new position to the set of visited points\n",
    "            visited_points.add(tuple(current_position))\n",
    "\n",
    "        points_list = list(visited_points)\n",
    "\n",
    "        # Generating random points\n",
    "        reference_pos = np.random.choice(a=N_grid_points, size=N_rand_points, replace=False)\n",
    "        reference_points = [list(points_list[i]) for i in reference_pos]\n",
    "\n",
    "        for point in reference_points:\n",
    "            point[0] += 10*np.random.random()\n",
    "            point[1] += 10*np.random.random()\n",
    "            visited_points.add(tuple(point))\n",
    "        \n",
    "\n",
    "        # Convert the set of visited points to a list\n",
    "        pos = np.array(list(visited_points))\n",
    "        pos = pos + 0.5*np.random.randn(pos.shape[0], pos.shape[1])\n",
    "\n",
    "        G = pygsp.graphs.NNGraph(pos,\n",
    "                                NNtype='radius',\n",
    "                                epsilon = 15, sigma = 100,\n",
    "                                center=False, rescale=False)\n",
    "        connected = G.is_connected()\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ramp(G, size):\n",
    "    # Auxiliary function for generating data.\n",
    "    # Returns array with \"size\" random ramps on the given graph. A ramp emulates regions with different behaviors.\n",
    "    # Direction of each ramp is randomly horizontal or vertical\n",
    "    # Slope, start and end positions of the ramp are random, within some constraints.\n",
    "    \n",
    "    pos = G.coords\n",
    "    min_slope = 0.1\n",
    "    max_slope = 1\n",
    "    \n",
    "    # Initialize an empty matrix to store ramp vectors\n",
    "    ramp_matrix = np.zeros((len(pos), size))\n",
    "\n",
    "    for i in range(size):\n",
    "        # Randomly choose the direction of the ramp (0, horizontal or 1, vertical)\n",
    "        if np.random.rand() > 0.5:\n",
    "            direction = 0\n",
    "        else:\n",
    "            direction = 1\n",
    "\n",
    "        # Generate a random slope within the specified maximum slope\n",
    "        slope = np.random.uniform(low=min_slope, high=max_slope)\n",
    "\n",
    "        # Calculate the minimum ramp length as a fraction of the peak-to-peak range\n",
    "        min_slope_dist = 0.25 * pos[:, direction].ptp()\n",
    "\n",
    "        # Determine the starting and ending positions\n",
    "        start = pos[:, direction].min() + pos[:, direction].ptp() * np.random.rand() * 0.5\n",
    "        end = start + min_slope_dist + (pos[:, direction].max() - start - min_slope_dist) * np.random.rand()\n",
    "\n",
    "        # Generate the ramp vector based on the slope and the position within the range\n",
    "        ramp = slope * (pos[:, direction] - start)\n",
    "\n",
    "        # Applying a mask that defines where the ramp exists\n",
    "        mask = (pos[:, direction] >= start) * (pos[:, direction] < end)\n",
    "        ramp = ramp * mask\n",
    "\n",
    "        # Set values beyond the end of the ramp to the maximum ramp value\n",
    "        ramp[pos[:, direction] >= end] = ramp.max()\n",
    "\n",
    "        # Store the ramp vector in the matrix\n",
    "        ramp_matrix[:, i] = ramp\n",
    "\n",
    "    return ramp_matrix\n",
    "\n",
    "def generate_smooth(G, size):\n",
    "    # Auxiliary function for generating data\n",
    "    # Returns array with \"size\" smooth signals by filtering white noise with the two first eigenvectors of the graph\n",
    "    \n",
    "    w, V = np.linalg.eigh(G.L.toarray())\n",
    "\n",
    "    # Low-pass filter\n",
    "    h = np.ones(len(w))\n",
    "    h[0] = 1\n",
    "    h[1] = 0.1\n",
    "    h[2:] = 0 \n",
    "\n",
    "    # Generating and filtering white noise to create a smooth graph signal\n",
    "    displacement = np.random.randn(G.N, size) \n",
    "    displacement = V @ np.diag(h) @ V.T @ displacement\n",
    "\n",
    "    # Normalizing signal: average power sum(x^2)/N = 1\n",
    "    displacement = np.sqrt(G.N)*displacement/(np.linalg.norm(displacement,axis=0))\n",
    "\n",
    "    return displacement\n",
    "\n",
    "\n",
    "def pick_non_adjacent(G, n):\n",
    "    # Auxiliary function for generating anomalies.\n",
    "    # Picks n nodes that are not adjacent to each other.\n",
    "    A = G.A.toarray()\n",
    "    np.fill_diagonal(A,1)\n",
    "\n",
    "    tries = 0\n",
    "\n",
    "    while tries < 100000:\n",
    "        tries+=1\n",
    "\n",
    "        fail = 0\n",
    "        possible_nodes= list(np.arange(G.N))\n",
    "        picked_nodes = []\n",
    "\n",
    "        for node in range(n):\n",
    "            if len(possible_nodes) == 0:\n",
    "                fail = 1\n",
    "            else:\n",
    "                pick = np.random.choice(possible_nodes)\n",
    "                picked_nodes.append(pick)\n",
    "                neighbors = np.where(A[pick,:])[0]\n",
    "                possible_nodes = list( set(possible_nodes) - set(neighbors)  )\n",
    "\n",
    "        if not fail:\n",
    "            return picked_nodes\n",
    "    \n",
    "    print('Maximum tries reached. Reduce n')\n",
    "\n",
    "def generate_anomaly(G, signal):\n",
    "    # signal is expected to be a matrix containing different signals in each column\n",
    "    # each column will receive different anomalies\n",
    "\n",
    "    anomaly_factor_min = 0.05\n",
    "    anomaly_factor_max = 0.15\n",
    "\n",
    "    N = signal.shape[0]\n",
    "    size = signal.shape[1]\n",
    "\n",
    "    # Defining number of anomalous sensors per signal. Each column of signal has a different number of anomalies\n",
    "    # This value is up to 5% the number of sensors (rounded up)\n",
    "    percentage_per_signal = 0.05*np.random.rand(size) + 1e-5 # value added to avoid zero\n",
    "    number_per_signal = np.ceil(percentage_per_signal*N)\n",
    "\n",
    "    anomalous_positions = [pick_non_adjacent(G,int(n)) for n in number_per_signal]\n",
    "\n",
    "    anomaly = np.zeros(signal.shape)\n",
    "    for i in range(size):\n",
    "        n = int(number_per_signal[i])\n",
    "        anomalous_positions = pick_non_adjacent(G,n)\n",
    "\n",
    "        anomaly[anomalous_positions, i] = signal[:,i].ptp()*np.random.uniform(low=anomaly_factor_min,\n",
    "                                                                              high=anomaly_factor_max,\n",
    "                                                                              size=n)*np.random.choice([+1,-1],size=n)\n",
    "        \n",
    "    label = np.zeros(signal.shape)\n",
    "    label[np.where(anomaly)] = 1\n",
    "\n",
    "    return anomaly, label\n",
    "\n",
    "def generate_data(G, size):    \n",
    "\n",
    "    # Generating healthy data\n",
    "    displacement = generate_smooth(G, size)\n",
    "    ramp = generate_ramp(G, size)\n",
    "    noise = np.sqrt(0.1)*np.random.randn(G.N,size) #noise power = 0.1, SNR = 20 dB\n",
    "\n",
    "    signal = displacement + ramp + noise\n",
    "\n",
    "    # Introducing anomalies\n",
    "    anomaly, label = generate_anomaly(G, signal)\n",
    "    signal = signal + anomaly\n",
    "    \n",
    "    return signal, label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igarss-REga8jWa-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
