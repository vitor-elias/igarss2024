{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vitorro/Library/Caches/pypoetry/virtualenvs/igarss-REga8jWa-py3.11/lib/python3.11/site-packages/pygsp/filters/simpletight.py:79: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if kerneltype is 'sf':\n",
      "/Users/vitorro/Library/Caches/pypoetry/virtualenvs/igarss-REga8jWa-py3.11/lib/python3.11/site-packages/pygsp/filters/simpletight.py:82: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif kerneltype is 'wavelet':\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KarateClub\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_networkx, grid\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reload\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyprojroot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m here\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import scipy as sp\n",
    "# import contextily as cx\n",
    "\n",
    "import torch\n",
    "import pygsp\n",
    "import optuna\n",
    "import joblib\n",
    "import gc\n",
    "import argparse\n",
    "import os\n",
    "import matplotlib\n",
    "\n",
    "from matplotlib.ticker import ScalarFormatter, StrMethodFormatter, FormatStrFormatter, FuncFormatter\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, auc\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from optuna.samplers import TPESampler\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn.models import GraphUNet\n",
    "from torch_geometric.nn import GCNConv, Sequential\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import KarateClub\n",
    "from torch_geometric.utils import to_networkx, grid\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from importlib import reload\n",
    "from pyprojroot import here\n",
    "ROOT_DIR = str(here())\n",
    "insar_path = ROOT_DIR + \"/data/raw/insar/\"\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "matplotlib.rcParams.update({'font.family': 'Times New Roman'})\n",
    "\n",
    "# import dario.models.mismatch_analysis as mma\n",
    "# mma = reload(mma)\n",
    "\n",
    "# Function definitions\n",
    "\n",
    "def plot_anim(outputs, epochs):\n",
    "    def generate_matrix(epoch):\n",
    "        out = outputs[epoch][2].detach().numpy().reshape(28,28)\n",
    "        inp = outputs[epoch][1].numpy().reshape(28,28)\n",
    "\n",
    "        out = np.c_[inp,out]\n",
    "        return out #np.abs(out-inp)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    def init():\n",
    "        ax.clear()\n",
    "        plt.close()\n",
    "\n",
    "    def update(frame):\n",
    "        matrix = generate_matrix(frame)  # Generate the matrix for the current frame\n",
    "        ax.imshow(matrix, cmap='gray', vmin=0, vmax=1)  # Update the plot with the new matrix\n",
    "        # Hide all ticks and tick labels\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_title(f'{frame}', fontdict={'color':'white'})\n",
    "        plt.close()\n",
    "\n",
    "    fps = 2\n",
    "    ani = FuncAnimation(fig, update, frames=range(epochs), interval=1000/fps, repeat=True, blit=False, init_func=init)\n",
    "    return ani\n",
    "\n",
    "def roc_params(metric, label, interp=True):\n",
    "    fpr = []\n",
    "    tpr = []\n",
    "    thr = []\n",
    "    thr_list = list(np.linspace(0, metric.max(),1001))\n",
    "\n",
    "    fp = 1\n",
    "    ind = 0\n",
    "    while fp > 0:\n",
    "        threshold = thr_list[ind]\n",
    "        ind += 1\n",
    "\n",
    "        y = (metric>threshold)\n",
    "        tn, fp, fn, tp = confusion_matrix(label, y).ravel()\n",
    "\n",
    "        fpr.append( fp/(tn + fp) )\n",
    "        tpr.append( tp/(tp + fn) )\n",
    "        thr.append( threshold )\n",
    "\n",
    "    while tp > 0:\n",
    "        threshold = thr_list[ind]\n",
    "        ind += 1\n",
    "        y = (metric>threshold)\n",
    "        tn, fp, fn, tp = confusion_matrix(label, y).ravel()\n",
    "\n",
    "    \n",
    "    fpr = fpr[::-1]\n",
    "    tpr = tpr[::-1]\n",
    "    thr = thr[::-1]\n",
    "\n",
    "    if interp:\n",
    "        fpr_base = np.linspace(0, 1, 101)\n",
    "        tpr = list(np.interp(fpr_base, fpr, tpr))\n",
    "        thr = list(np.interp(fpr_base, fpr, thr))\n",
    "        fpr = list(fpr_base)\n",
    "\n",
    "    fpr.insert(0, 0)\n",
    "    tpr.insert(0, 0)\n",
    "    thr.insert(0, threshold)\n",
    "\n",
    "    return tpr, fpr, thr\n",
    "\n",
    "def compute_auc(tpr, fpr):\n",
    "    auc = 0\n",
    "    for i in range(1, len(fpr)):\n",
    "        auc += (fpr[i] - fpr[i - 1]) * (tpr[i] + tpr[i - 1]) / 2\n",
    "    return auc\n",
    "\n",
    "# def detection(df_metrics, column_name='wse', threshold_min=1000, threshold_max=np.inf, selector='group',\n",
    "#               detection_param='detection_sum', detection_param_threshold=None):\n",
    "#     # df_relevant contains data from nodes that, at some point, have lower<=wse<=upper, and their neighbors.\n",
    "#     # nodes are put into groups if they are close to each other.\n",
    "\n",
    "#     if detection_param_threshold is None:\n",
    "#         detection_param_threshold = df_metrics.timestamp.nunique()//2\n",
    "\n",
    "#     df_relevant = mma.relevant_neighborhood(df_metrics, column_name=column_name,\n",
    "#                                             lower=threshold_min, upper=threshold_max,\n",
    "#                                             only_relevant=True, return_df=True, plot=False, filter_dates=False)\n",
    "\n",
    "#     # Treating disconnected nodes as individual groups. Assining new values\n",
    "#     new_group_values = df_relevant.query('group==0').pid.factorize()[0] + df_relevant.group.max()+1\n",
    "#     df_relevant.loc[df_relevant.group==0, 'group'] = new_group_values\n",
    "\n",
    "\n",
    "#     df_relevant['detection'] = (df_relevant[column_name]>=threshold_min) & (df_relevant[column_name]<=threshold_max)\n",
    "#     df_detection = df_relevant.groupby('pid').agg({column_name:['max','mean'],\n",
    "#                                                     'detection':['sum',mma.consecutive_ones],\n",
    "#                                                     'group':'mean'}).reset_index()\n",
    "\n",
    "#     df_detection.columns = [f\"{level1}_{level2}\" if level2 else level1 for level1, level2 in df_detection.columns]\n",
    "#     df_detection.rename({'group_mean':'group'}, axis=1, inplace=True)\n",
    "\n",
    "#     query = f'{detection_param}>{detection_param_threshold}'\n",
    "#     selected = df_detection.query(query)[selector].unique()\n",
    "\n",
    "#     return df_relevant, selected\n",
    "\n",
    "# def skew(df):\n",
    "#     return np.abs(sp.stats.skew(df.mean_velocity))\n",
    "\n",
    "\n",
    "# def compute_metric(df_test, cut=2, radius=15):\n",
    "\n",
    "#     df_metrics = []\n",
    "#     for cluster in sorted(df_test.cluster.unique()):\n",
    "\n",
    "#         df, nodes = mma.treat_nodes(df_test.query('cluster==@cluster'))\n",
    "#         G, nodes['subgraph'] = mma.NNGraph(nodes, radius=radius, subgraphs=True)\n",
    "\n",
    "#         df_metrics_cluster = []\n",
    "#         for sub_index in sorted(nodes.subgraph.unique())[1:]:\n",
    "\n",
    "#             subnodes = nodes.query('subgraph==@sub_index').copy()\n",
    "#             subdf = df[df.pid.isin(subnodes.pid)].copy()\n",
    "\n",
    "#             G = mma.NNGraph(subnodes, radius=radius)\n",
    "\n",
    "#             w, V = np.linalg.eigh(G.L.toarray())\n",
    "#             wh = np.ones(G.N)\n",
    "#             wh[w<cut] = 0\n",
    "#             Hh = V @ np.diag(wh) @ V.T\n",
    "\n",
    "#             smoothed = subdf[['pid', 'timestamp', 'smoothed' ]].pivot(index='pid', columns='timestamp')\n",
    "\n",
    "#             subdf['hf'] = np.abs((Hh @ smoothed.values).reshape((-1,), order='C'))\n",
    "\n",
    "#             df_metrics_cluster.append(subdf)\n",
    "\n",
    "#         df_metrics_cluster = pd.concat(df_metrics_cluster)\n",
    "#         df_metrics.append(df_metrics_cluster)\n",
    "\n",
    "#     df_metrics = pd.concat(df_metrics)\n",
    "#     return df_metrics\n",
    "\n",
    "\n",
    "# def hfilter(G, cut=2):\n",
    "#     L = G.L.toarray()\n",
    "#     w, V = np.linalg.eigh(L)\n",
    "#     wh = np.ones(G.N)\n",
    "#     wh[w<cut] = 0\n",
    "#     Hh = V @ np.diag(wh) @ V.T\n",
    "#     return Hh\n",
    "\n",
    "# def matplotlib_roc(save=None, ax=None):\n",
    "#     matplotlib.rcParams.update({'font.size': 20})\n",
    "#     matplotlib.rcParams.update({'font.family': 'Times New Roman'})\n",
    "\n",
    "#     if ax is None:\n",
    "#         fig, ax = plt.subplots(figsize=(12,5))\n",
    "#     # sc = ax.scatter(fpr, tpr, c=thr, cmap='viridis', label='Threshold')\n",
    "#     sc = ax.plot(fpr, tpr, linestyle='dotted', linewidth=1, color='black')\n",
    "\n",
    "#     # # Colorbar\n",
    "#     # cbar = plt.colorbar(sc, ax=ax)\n",
    "#     # cbar.set_label('Threshold', rotation=270, labelpad=15)\n",
    "\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     # plt.grid()\n",
    "#     # plt.tight_layout()\n",
    "\n",
    "#     if save is not None:\n",
    "#         plt.savefig(save, transparent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.load(ROOT_DIR+\"/models/outputs/optuna_wse/optimization_logs_2.pkl\").trials_dataframe().sort_values('value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc = pd.read_parquet(ROOT_DIR+\"/data/interim/df_Porsgrunn_A1L2B.parq\")\n",
    "df_orig = pd.read_csv(insar_path+\"/A1/L2B_117_0350_IW3_VV.csv\") # New Prorsgrunn A1\n",
    "df = df_orig.copy()\n",
    "\n",
    "lat_min, lat_max, lon_min, lon_max = (59.10, 59.20, 9.55, 9.74) # 1 - Porsgrunn\n",
    "df = df[ (df.longitude>lon_min) & (df.longitude<=lon_max) &\n",
    "            (df.latitude>lat_min) & (df.latitude<=lat_max)  ]\n",
    "\n",
    "# Selection relevant columns\n",
    "date_cols = sorted([col for col in df.columns if \"20\" in col]) #columns named after timestamps\n",
    "keep_cols = date_cols #list with variables to keep from dataframe\n",
    "id_cols = ['pid', 'latitude', 'longitude', 'easting', 'northing', 'mean_velocity']\n",
    "keep_cols.extend(id_cols)\n",
    "df = df[keep_cols]  #replacing old df for memory efficiency\n",
    "# df_originals.append(df)\n",
    "\n",
    "# Formatting from wide to tall dataframe\n",
    "# Uses a single column for timestamp and a column for displacement\n",
    "# Number of rows = number of pixels * number of timestamps\n",
    "df = df.melt(id_vars=id_cols, value_vars=date_cols,\n",
    "                var_name='timestamp', value_name='displacement').sort_values('pid')\n",
    "df.timestamp = pd.to_datetime(df.timestamp)\n",
    "\n",
    "# RETRO: based on gap before 2016.06\n",
    "df = df[df.timestamp>='2016-06-01'].copy()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.sort_values(['pid','timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "matplotlib.rcParams.update({'font.family': 'Times New Roman'})\n",
    "\n",
    "id = df_proc.pid.unique()[0]\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.scatter(df.query('pid==@id').timestamp, df.query('pid==@id').displacement, s=15)\n",
    "ax.plot(df_proc.query('pid==@id').timestamp, df_proc.query('pid==@id').smoothed, color='red')\n",
    "ax.set_xlabel('Timestamp')\n",
    "ax.set_ylabel('Ground displacement [mm]')\n",
    "ax.legend(['Original','Preprocessed'])\n",
    "plt.grid(which='both')\n",
    "plt.tight_layout()\n",
    "plt.savefig(ROOT_DIR+f\"/models/outputs/figs/ReportESA/preprocessing\", transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc = pd.read_parquet(ROOT_DIR+\"/data/interim/df_Malmo_D1old.parq\")\n",
    "df_orig = pd.read_csv(insar_path+\"066_0742_iw1_vv.csv\") # Old Malmo\n",
    "df = df_orig.copy()\n",
    "\n",
    "lat_min, lat_max, lon_min, lon_max = (55.55, 55.58, 12.9,13.1) # Malmo\n",
    "# lat_min, lat_max, lon_min, lon_max = (59.10, 59.20, 9.55, 9.74) # 1 - Porsgrunn\n",
    "df = df[ (df.longitude>lon_min) & (df.longitude<=lon_max) &\n",
    "            (df.latitude>lat_min) & (df.latitude<=lat_max)  ]\n",
    "\n",
    "# Selection relevant columns\n",
    "date_cols = sorted([col for col in df.columns if \"20\" in col]) #columns named after timestamps\n",
    "keep_cols = date_cols #list with variables to keep from dataframe\n",
    "id_cols = ['pid', 'latitude', 'longitude', 'easting', 'northing', 'mean_velocity']\n",
    "keep_cols.extend(id_cols)\n",
    "df = df[keep_cols]  #replacing old df for memory efficiency\n",
    "# df_originals.append(df)\n",
    "\n",
    "# Formatting from wide to tall dataframe\n",
    "# Uses a single column for timestamp and a column for displacement\n",
    "# Number of rows = number of pixels * number of timestamps\n",
    "df = df.melt(id_vars=id_cols, value_vars=date_cols,\n",
    "                var_name='timestamp', value_name='displacement').sort_values('pid')\n",
    "df.timestamp = pd.to_datetime(df.timestamp)\n",
    "\n",
    "# RETRO: based on gap before 2016.06\n",
    "df = df[df.timestamp>='2016-06-01'].copy()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.sort_values(['pid','timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "matplotlib.rcParams.update({'font.family': 'Times New Roman'})\n",
    "\n",
    "id = df_proc.pid.unique()[0]\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.scatter(df.query('pid==@id').timestamp, df.query('pid==@id').displacement, s=15)\n",
    "ax.plot(df_proc.query('pid==@id').timestamp, df_proc.query('pid==@id').smoothed, color='red')\n",
    "ax.set_xlabel('Timestamp')\n",
    "ax.set_ylabel('Ground displacement [mm]')\n",
    "ax.legend(['Original','Preprocessed'])\n",
    "plt.grid(which='both')\n",
    "plt.tight_layout()\n",
    "plt.savefig(ROOT_DIR+f\"/models/outputs/figs/ReportESA/preprocessing_malmo\", transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "df_test = df.drop_duplicates('pid')\n",
    "# df_test = df_test.query(('latitude>58.147 and latitude<58.149 and longitude>8.02 and longitude<8.024'))\n",
    "# df_test = df_test[df_test.pid.isin(np.random.choice(df_test.pid.unique(), size=200))]\n",
    "\n",
    "\n",
    "Graph = mma.NNGraph(df_test, radius=15, plotting_params= {'edge_color':'darkgray', 'edge_width':1.5,'vertex_color':'black', 'vertex_size':5})\n",
    "Graph.coords = df_test[['longitude','latitude']].values\n",
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "# ax.scatter(df_test.longitude, df_test.latitude)\n",
    "\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "Graph.plot(ax=ax, plot_name='')\n",
    "cx.add_basemap(ax, crs='epsg:4326', source=cx.providers.OpenStreetMap.Mapnik)\n",
    "# plt.savefig(ROOT_DIR+\"/models/outputs/figs/ReportESA/graph.png\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "df_test = df.drop_duplicates('pid')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "# colors = df_test.mean_velocity.astype('category').cat.codes\n",
    "ax.scatter(df_test.longitude, df_test.latitude, s=0.1, cmap='Greys')\n",
    "\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "# Graph.plot(ax=ax, plot_name='')\n",
    "cx.add_basemap(ax, crs='epsg:4326', source=cx.providers.OpenStreetMap.Mapnik)\n",
    "plt.savefig(ROOT_DIR+\"/models/outputs/figs/ReportESA/Porsgrunn.png\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAPH UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(argparse.Namespace):\n",
    "    n_epochs = [20,40,60,80]\n",
    "    n_trials = 45\n",
    "    learning_rate = [1e-3, 1e-2, 1e-1]\n",
    "    penalty_rate = [1e-15, 1e-5, 1e-1]\n",
    "    hidden_channels = [2, 3, 5]\n",
    "    depth = [2, 3, 5]\n",
    "    pool_ratios = [0.2, 0.5, 0.7]\n",
    "\n",
    "    log_dir=ROOT_DIR + '/models/outputs/optuna_gunet/'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "def roc_params(metric, label, interp=True):\n",
    "    fpr = []\n",
    "    tpr = []\n",
    "    thr = []\n",
    "    thr_list = list(np.linspace(0, metric.max(),1001))\n",
    "\n",
    "    fp = 1\n",
    "    ind = 0\n",
    "    while fp > 0:\n",
    "        threshold = thr_list[ind]\n",
    "        ind += 1\n",
    "\n",
    "        y = (metric>threshold)\n",
    "        tn, fp, fn, tp = confusion_matrix(label, y).ravel()\n",
    "\n",
    "        fpr.append( fp/(tn + fp) )\n",
    "        tpr.append( tp/(tp + fn) )\n",
    "        thr.append( threshold )\n",
    "\n",
    "    while tp > 0:\n",
    "        threshold = thr_list[ind]\n",
    "        ind += 1\n",
    "        y = (metric>threshold)\n",
    "        tn, fp, fn, tp = confusion_matrix(label, y).ravel()\n",
    "\n",
    "    \n",
    "    fpr = fpr[::-1]\n",
    "    tpr = tpr[::-1]\n",
    "    thr = thr[::-1]\n",
    "\n",
    "    if interp:\n",
    "        fpr_base = np.linspace(0, 1, 101)\n",
    "        tpr = list(np.interp(fpr_base, fpr, tpr))\n",
    "        thr = list(np.interp(fpr_base, fpr, thr))\n",
    "        fpr = list(fpr_base)\n",
    "\n",
    "    fpr.insert(0, 0)\n",
    "    tpr.insert(0, 0)\n",
    "    thr.insert(0, threshold)\n",
    "\n",
    "    return tpr, fpr, thr\n",
    "\n",
    "def compute_auc(tpr, fpr):\n",
    "    auc = 0\n",
    "    for i in range(1, len(fpr)):\n",
    "        auc += (fpr[i] - fpr[i - 1]) * (tpr[i] + tpr[i - 1]) / 2\n",
    "    return auc\n",
    "\n",
    "def train_model(model, n_epochs, learning_rate, penalty_rate):\n",
    "\n",
    "    loss_function = torch.nn.MSELoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                lr = learning_rate,\n",
    "                                weight_decay = penalty_rate)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    auc = []\n",
    "\n",
    "    for seed in range(10):\n",
    "\n",
    "        print(f'seed:{seed}')\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        G = mma.synth_graph(seed=seed)\n",
    "        edge_index = torch.tensor(np.array(np.nonzero(G.A.toarray())), dtype=torch.long)\n",
    "\n",
    "        data, label = mma.create_data(G, anomaly=0.1, size=25, seed=seed,\n",
    "                                    noise_var=1e-5, eigs=1, signal_power=1e-6)\n",
    "\n",
    "        data = scaler.fit_transform(data)\n",
    "        label_vector = label.reshape((-1,), order='F')\n",
    "\n",
    "        error = []\n",
    "        \n",
    "        for snap in range(data.shape[1]):\n",
    "\n",
    "            # GRAPH UNET PART\n",
    "            x = torch.Tensor(data[:,snap]).reshape(-1,1)\n",
    "\n",
    "            model.reset_parameters()\n",
    "\n",
    "            epochs = n_epochs\n",
    "            outputs = []\n",
    "            losses = []\n",
    "            for epoch in range(epochs):\n",
    "                        \n",
    "                reconstructed = model(x, edge_index)     # Output of Autoencoder\n",
    "                loss = loss_function(reconstructed, x)    # Calculating the loss function\n",
    "                \n",
    "                optimizer.zero_grad() # The gradients are set to zero,\n",
    "                loss.backward() # the gradient is computed and stored.\n",
    "                optimizer.step() # .step() performs parameter update\n",
    "\n",
    "                # Storing the losses in a list for plotting\n",
    "                losses.append(loss)\n",
    "                outputs.append((epoch, x, reconstructed))\n",
    "\n",
    "            error_snap = np.abs(reconstructed.detach() - x).numpy().flatten()\n",
    "            error.extend(error_snap)\n",
    "            \n",
    "        error = np.array(error).reshape((-1,))\n",
    "        tpr, fpr, thr = roc_params(error, label_vector, interp=True)\n",
    "        auc.append(compute_auc(tpr,fpr))\n",
    "\n",
    "    return np.mean(auc)        \n",
    "    \n",
    "    \n",
    "def objective(trial):\n",
    "    gc.collect()\n",
    "\n",
    "    n_epochs = trial.suggest_categorical('n_epochs', args.n_epochs)\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', args.learning_rate)\n",
    "    penalty_rate = trial.suggest_categorical('penalty_rate', args.penalty_rate)\n",
    "    hidden_channels = trial.suggest_categorical('hidden_channels', args.hidden_channels)\n",
    "    depth = trial.suggest_categorical('depth', args.depth)\n",
    "    pool_ratios = trial.suggest_categorical('pool_ratios', args.pool_ratios)\n",
    "\n",
    "    print(f\"INFO: Trial number: {trial.number}\")\n",
    "    print(f\"INFO: Learning rate: {learning_rate}\")\n",
    "    print(f\"INFO: Penalty rate: {penalty_rate}\")\n",
    "    print(f\"INFO: Hidden_channels: {hidden_channels}\")\n",
    "    print(f\"INFO: Depth: {depth}\")\n",
    "    print(f\"INFO: Pool ratios: {pool_ratios}\")\n",
    "    print(f\"INFO: n_epochs: {n_epochs}\")\n",
    "\n",
    "    model = GraphUNet(1, hidden_channels, 1, depth, pool_ratios)\n",
    "\n",
    "    return train_model(model, n_epochs, learning_rate, penalty_rate)\n",
    "\n",
    "\n",
    "if not os.path.exists(args.log_dir):\n",
    "    os.makedirs(args.log_dir,exist_ok=True)\n",
    "\n",
    "study = optuna.create_study(sampler=TPESampler(),\n",
    "                            direction='maximize',\n",
    "                            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=24, interval_steps=6))\n",
    "\n",
    "log_file = args.log_dir + 'optimization_logs.pkl'\n",
    "if os.path.isfile(log_file):\n",
    "    study = joblib.load(log_file)\n",
    "\n",
    "study.optimize(objective, n_trials=args.n_trials, gc_after_trial=True)\n",
    "joblib.dump(study, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = joblib.load('/Users/vitorro/Repositories/dario/models/outputs/optuna_gunet/optimization_logs.pkl')\n",
    "study.trials_dataframe().query('params_n_epochs==80').sort_values('value').tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "matplotlib.rcParams.update({'font.family': 'Times New Roman'})\n",
    "\n",
    "learning_rate = 0.1\n",
    "penalty_rate = 1e-5\n",
    "n_epochs = 80\n",
    "\n",
    "hidden_channels = 5\n",
    "depth = 2\n",
    "pool_ratios = 0.5\n",
    "\n",
    "model = GraphUNet(1, hidden_channels, 1, depth, pool_ratios)\n",
    "\n",
    "loss_function = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr = learning_rate,\n",
    "                            weight_decay = penalty_rate)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "auc = []\n",
    "\n",
    "test_seed = 1001\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "\n",
    "for seed in range(10):\n",
    "\n",
    "    print(f'seed:{seed}')\n",
    "\n",
    "    seed = seed+test_seed\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    G = mma.synth_graph(seed=seed)\n",
    "    edge_index = torch.tensor(np.array(np.nonzero(G.A.toarray())), dtype=torch.long)\n",
    "\n",
    "    data, label = mma.create_data(G, anomaly=0.1, size=25, seed=seed,\n",
    "                                noise_var=1e-5, eigs=1, signal_power=1e-6)\n",
    "\n",
    "    data = scaler.fit_transform(data)\n",
    "    label_vector = label.reshape((-1,), order='F')\n",
    "\n",
    "    error = []\n",
    "    \n",
    "    for snap in range(data.shape[1]):\n",
    "\n",
    "        # GRAPH UNET PART\n",
    "        x = torch.Tensor(data[:,snap]).reshape(-1,1)\n",
    "\n",
    "        model.reset_parameters()\n",
    "\n",
    "        epochs = n_epochs\n",
    "        outputs = []\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "                    \n",
    "            reconstructed = model(x, edge_index)     # Output of Autoencoder\n",
    "            loss = loss_function(reconstructed, x)    # Calculating the loss function\n",
    "            \n",
    "            optimizer.zero_grad() # The gradients are set to zero,\n",
    "            loss.backward() # the gradient is computed and stored.\n",
    "            optimizer.step() # .step() performs parameter update\n",
    "\n",
    "            # Storing the losses in a list for plotting\n",
    "            losses.append(loss)\n",
    "            outputs.append((epoch, x, reconstructed))\n",
    "\n",
    "        error_snap = np.abs(reconstructed.detach() - x).numpy().flatten()\n",
    "        error.extend(error_snap)\n",
    "        \n",
    "    error = np.array(error).reshape((-1,))\n",
    "    tpr, fpr, thr = roc_params(error, label_vector, interp=True)\n",
    "    auc.append(compute_auc(tpr,fpr))\n",
    "    matplotlib_roc(ax=ax)\n",
    "\n",
    "# fig = px.scatter(x=fpr, y=tpr, color=thr,\n",
    "#            labels={'x':'false positive rate', 'y':'true positive rate', 'color':'threshold'},\n",
    "#            width=700, height=500)\n",
    "# fig.update_traces(line={'dash':'dot'}, mode='lines+markers')\n",
    "# fig.show()\n",
    "\n",
    "# matplotlib_roc(save=ROOT_DIR+\"/models/outputs/figs/ReportESA_synth_roc_gunet.png\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(ROOT_DIR+\"/models/outputs/figs/ReportESA/synth_roc_gunet.png\", transparent=True)\n",
    "print(np.mean(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAPH FILTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(argparse.Namespace):\n",
    "    cut = np.arange(0,8,0.5).tolist()\n",
    "    n_trials = len(cut)\n",
    "\n",
    "    log_dir=ROOT_DIR + '/models/outputs/optuna_gfilter/'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "def train_model_gsp(cut):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    auc = []\n",
    "\n",
    "    for seed in range(10):\n",
    "\n",
    "        print(f'seed:{seed}')\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        G = mma.synth_graph(seed=seed)\n",
    "        edge_index = torch.tensor(np.array(np.nonzero(G.A.toarray())), dtype=torch.long)\n",
    "\n",
    "        data, label = mma.create_data(G, anomaly=0.1, size=25, seed=seed,\n",
    "                                    noise_var=1e-5, eigs=1, signal_power=1e-6)\n",
    "\n",
    "        data = scaler.fit_transform(data)\n",
    "        label_vector = label.reshape((-1,), order='F')\n",
    "\n",
    "        Hh = hfilter(G, cut)\n",
    "\n",
    "        error = []\n",
    "        \n",
    "        for snap in range(data.shape[1]):\n",
    "\n",
    "            error.extend(np.abs(Hh @ data[:,snap]))\n",
    "                        \n",
    "        error = np.array(error).reshape((-1,))\n",
    "        tpr, fpr, thr = roc_params(error, label_vector, interp=True)\n",
    "        auc.append(compute_auc(tpr,fpr))\n",
    "\n",
    "    return np.mean(auc)        \n",
    "    \n",
    "    \n",
    "def objective(trial):\n",
    "    gc.collect()\n",
    "\n",
    "    cut = trial.suggest_categorical('cut',args.cut)\n",
    "\n",
    "    print(f\"INFO: Trial number: {trial.number}\")\n",
    "    print(f\"INFO: cut: {cut}\")\n",
    "\n",
    "    return train_model_gsp(cut)\n",
    "\n",
    "\n",
    "if not os.path.exists(args.log_dir):\n",
    "    os.makedirs(args.log_dir,exist_ok=True)\n",
    "\n",
    "study = optuna.create_study(sampler=TPESampler(),\n",
    "                            direction='maximize',\n",
    "                            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=24, interval_steps=6))\n",
    "\n",
    "log_file = args.log_dir + 'optimization_logs.pkl'\n",
    "if os.path.isfile(log_file):\n",
    "    study = joblib.load(log_file)\n",
    "\n",
    "study.optimize(objective, n_trials=args.n_trials, gc_after_trial=True)\n",
    "joblib.dump(study, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = joblib.load('/Users/vitorro/Repositories/dario/models/outputs/optuna_gfilter/optimization_logs.pkl')\n",
    "study.trials_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = 1.5\n",
    "\n",
    "scaler = StandardScaler()\n",
    "auc = []\n",
    "\n",
    "test_seed = 1001\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "\n",
    "for seed in range(10):\n",
    "\n",
    "    print(f'seed:{seed}')\n",
    "\n",
    "    seed = seed+test_seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    G = mma.synth_graph(seed=seed)\n",
    "    edge_index = torch.tensor(np.array(np.nonzero(G.A.toarray())), dtype=torch.long)\n",
    "\n",
    "    data, label = mma.create_data(G, anomaly=0.1, size=25, seed=seed,\n",
    "                                noise_var=1e-5, eigs=1, signal_power=1e-6)\n",
    "\n",
    "    data = scaler.fit_transform(data)\n",
    "    label_vector = label.reshape((-1,), order='F')\n",
    "\n",
    "    Hh = hfilter(G, cut)\n",
    "\n",
    "    error = []\n",
    "    \n",
    "    for snap in range(data.shape[1]):\n",
    "\n",
    "        filtered = np.abs(Hh @ data[:,snap])\n",
    "        error.extend(filtered)\n",
    "                    \n",
    "    error = np.array(error).reshape((-1,))\n",
    "    tpr, fpr, thr = roc_params(error, label_vector, interp=True)\n",
    "    auc.append(compute_auc(tpr,fpr))\n",
    "    matplotlib_roc(ax=ax)\n",
    "\n",
    "# fig = px.scatter(x=fpr, y=tpr, color=thr,\n",
    "#            labels={'x':'false positive rate', 'y':'true positive rate', 'color':'threshold'},\n",
    "#            width=700, height=500)\n",
    "# fig.update_traces(line={'dash':'dot'}, mode='lines+markers')\n",
    "# fig.show()\n",
    "# matplotlib_roc(save=ROOT_DIR+\"/models/outputs/figs/ReportESA_synth_roc_gfilter.png\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(ROOT_DIR+\"/models/outputs/figs/ReportESA/synth_roc_gfilter.png\", transparent=True)\n",
    "print(np.mean(auc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'df_Trondheim_A1L2B'\n",
    "df_orig = pd.read_parquet(ROOT_DIR+f\"/data/interim/{dataset}.parq\")\n",
    "\n",
    "th1 = 70 # round 1 threshold\n",
    "th2 = 40 # round 2 threshold\n",
    "th_hits = 10 # number of timestamps hitting threshold to assign anomaly\n",
    "\n",
    "cut = 2 # frequency cut\n",
    "radius = 20 # radius for constructing NNGraph\n",
    "\n",
    "df = df_orig.copy()\n",
    "\n",
    "# round 1\n",
    "df_metrics = compute_metric(df, cut, radius)\n",
    "df_detection, selected = detection(df_metrics, column_name='hf', threshold_min=th1, selector='pid',\n",
    "                                   detection_param='detection_sum', detection_param_threshold=th_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relevant = mma.relevant_neighborhood(df_metrics, column_name='hf', lower=10, upper=14, zoom=11, range_meters=15,\n",
    "                          only_relevant=False, filter_dates=False, by_max=True, return_df=True) #lower 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "def plot_pixel_data(df, y='smoothed', animation_frame=None, range_y=None, figsize=(12, 8)):\n",
    "    matplotlib.rcParams.update({'font.size': 20})\n",
    "    matplotlib.rcParams.update({'font.family': 'Times New Roman'})\n",
    "\n",
    "    if animation_frame:\n",
    "        # Handle the case with animation_frame\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        unique_pids = df['pid'].unique()\n",
    "        cmap = plt.get_cmap('tab10')\n",
    "        \n",
    "        for pid in unique_pids:\n",
    "            subset = df[df['pid'] == pid]\n",
    "            ax.plot(subset['timestamp'], subset[y], label=f'PID {pid}', marker='o', color=cmap(pid))\n",
    "        \n",
    "        ax.set_xlabel('Timestamp')\n",
    "        ax.set_ylabel(y)\n",
    "        ax.legend(title='PID', loc='upper right')\n",
    "        ax.grid(True)\n",
    "        \n",
    "        if range_y:\n",
    "            ax.set_ylim(range_y)\n",
    "        \n",
    "        plt.title('Pixel Data Over Time')\n",
    "        \n",
    "    else:\n",
    "        # Handle the case without animation_frame\n",
    "        id_list = df['pid'].unique()\n",
    "        pivot_df = df.pivot(index='timestamp', columns='pid', values=y)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        pivot_df.plot(ax=ax, colormap='tab10')\n",
    "        \n",
    "        ax.set_xlabel('Timestamp')\n",
    "        ax.set_ylabel('Ground displacement [mm]')\n",
    "        ax.legend(title='ID')\n",
    "        ax.grid(True)\n",
    "\n",
    "        ax.legend().remove()\n",
    "\n",
    "        # Format x-axis labels to show only the year\n",
    "        date_format = mdates.DateFormatter('%Y')\n",
    "        ax.xaxis.set_major_formatter(date_format)\n",
    "        \n",
    "        # Set major locator to show only one tick per year\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator(base=1))\n",
    "        \n",
    "        # Set minor locator to show ticks for every month (adjust interval as needed)\n",
    "        # ax.xaxis.set_minor_locator(mdates.MonthLocator(interval=1))\n",
    "        \n",
    "        if range_y:\n",
    "            ax.set_ylim(range_y)\n",
    "    plt.tight_layout()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "matplotlib.rcParams.update({'font.family': 'Times New Roman'})\n",
    "\n",
    "# df_plot = df_relevant.query('group==248')\n",
    "\n",
    "# legend = []\n",
    "# fig, ax = plt.subplots(figsize=(12,9))\n",
    "# for pid in df_plot.pid.unique():\n",
    "#     ax.plot(df_plot.query('pid==@pid').timestamp, df_plot.query('pid==@pid').smoothed)\n",
    "#     legend.append(pid)\n",
    "\n",
    "# plt.grid(which='both')\n",
    "# ax.set_xlabel('Timestamp')\n",
    "# ax.set_ylabel('Ground displacement [mm]')\n",
    "# # ax.legend(legend)\n",
    "# # plt.savefig(ROOT_DIR+f\"/models/outputs/figs/ReportESA/fault_example.png\", transparent=True)\n",
    "# plt.show() \n",
    "\n",
    "plot_pixel_data(df_relevant.query('group==248'), y='smoothed', animation_frame=None, range_y=None, figsize=(5, 5))\n",
    "# plt.savefig(ROOT_DIR+\"/models/outputs/figs/ReportESA/IGARSS_fault_example.png\", transparent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(2, 5.5, 0.5).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(argparse.Namespace):\n",
    "    cut = np.arange(2, 5.5, 0.5).tolist()\n",
    "    decay = [1, 1.5, 2, 2.5, 3, 3.5]\n",
    "    n_trials = 20\n",
    "\n",
    "    log_dir=ROOT_DIR + '/models/outputs/optuna_wse/'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "def train_model_gsp(cut, decay):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    auc = []\n",
    "\n",
    "    for seed in range(10):\n",
    "\n",
    "        print(f'seed:{seed}')\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        G = mma.synth_graph(seed=seed)\n",
    "        nodes = pd.DataFrame({'easting':G.coords[:,0], 'northing':G.coords[:,1]})\n",
    "\n",
    "\n",
    "        data, label = mma.create_data(G, anomaly=0.1, size=25, seed=seed,\n",
    "                                    noise_var=1e-5, eigs=1, signal_power=1e-6)\n",
    "\n",
    "        data = scaler.fit_transform(data)\n",
    "        label_vector = label.reshape((-1,), order='F')\n",
    "\n",
    "        error = []\n",
    "        \n",
    "        for snap in range(data.shape[1]):\n",
    "\n",
    "            wse = mma.wse(nodes, x=data[:,snap].reshape((-1,1)), decay=decay, cut=cut)\n",
    "\n",
    "            error.extend(wse)\n",
    "                        \n",
    "        error = np.array(error).reshape((-1,))\n",
    "        tpr, fpr, thr = roc_params(error, label_vector, interp=True)\n",
    "        auc.append(compute_auc(tpr,fpr))\n",
    "\n",
    "    return np.mean(auc)\n",
    "    \n",
    "    \n",
    "def objective(trial):\n",
    "    gc.collect()\n",
    "\n",
    "    cut = trial.suggest_categorical('cut', args.cut)\n",
    "    decay = trial.suggest_categorical('decay', args.decay)\n",
    "\n",
    "    print(f\"INFO: Trial number: {trial.number}\")\n",
    "    print(f\"INFO: cut: {cut}\")\n",
    "    print(f\"INFO: decay: {decay}\")\n",
    "\n",
    "    return train_model_gsp(cut, decay)\n",
    "\n",
    "\n",
    "if not os.path.exists(args.log_dir):\n",
    "    os.makedirs(args.log_dir,exist_ok=True)\n",
    "\n",
    "study = optuna.create_study(sampler=TPESampler(),\n",
    "                            direction='maximize',\n",
    "                            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=24, interval_steps=6))\n",
    "\n",
    "log_file = args.log_dir + 'optimization_logs_2.pkl'\n",
    "if os.path.isfile(log_file):\n",
    "    study = joblib.load(log_file)\n",
    "\n",
    "study.optimize(objective, n_trials=args.n_trials, gc_after_trial=True)\n",
    "joblib.dump(study, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = 4.0\n",
    "decay = 1\n",
    "\n",
    "scaler = StandardScaler()\n",
    "auc = []\n",
    "\n",
    "test_seed = 1001\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "\n",
    "for seed in range(10):\n",
    "\n",
    "    print(f'seed:{seed}')\n",
    "\n",
    "    seed = seed+test_seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    G = mma.synth_graph(seed=seed)\n",
    "    nodes = pd.DataFrame({'easting':G.coords[:,0], 'northing':G.coords[:,1]})\n",
    "\n",
    "    data, label = mma.create_data(G, anomaly=0.1, size=25, seed=seed,\n",
    "                                noise_var=1e-5, eigs=1, signal_power=1e-6)\n",
    "\n",
    "    data = scaler.fit_transform(data)\n",
    "    label_vector = label.reshape((-1,), order='F')\n",
    "\n",
    "    error = []\n",
    "    \n",
    "    for snap in range(data.shape[1]):\n",
    "\n",
    "        wse = mma.wse(nodes, x=data[:,snap].reshape((-1,1)), decay=decay, cut=cut)\n",
    "        error.extend(wse)\n",
    "                    \n",
    "    error = np.array(error).reshape((-1,))\n",
    "    tpr, fpr, thr = roc_params(error, label_vector, interp=True)\n",
    "    auc.append(compute_auc(tpr,fpr))\n",
    "    matplotlib_roc(ax=ax)\n",
    "\n",
    "# fig = px.scatter(x=fpr, y=tpr, color=thr,\n",
    "#            labels={'x':'false positive rate', 'y':'true positive rate', 'color':'threshold'},\n",
    "#            width=700, height=500)\n",
    "# fig.update_traces(line={'dash':'dot'}, mode='lines+markers')\n",
    "# fig.show()\n",
    "# matplotlib_roc(save=ROOT_DIR+\"/models/outputs/figs/ReportESA_synth_roc_wse.png\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(ROOT_DIR+\"/models/outputs/figs/ReportESA/synth_roc_wse.png\", transparent=True)\n",
    "print(np.mean(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes['data'] = data[:,snap]\n",
    "nodes['label'] = label[:,snap]\n",
    "nodes['wse'] = wse\n",
    "nodes['gunet'] = error_snap\n",
    "nodes['hf'] = filtered\n",
    "# nodes = nodes.reset_index(names='pid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mma.visualise_mismatch_map(nodes, color='label', size='wse', zoom=17.8, recenter=[4000000,0000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "colors = nodes['label'].astype('category').cat.codes\n",
    "ax.scatter(nodes['easting'], nodes['northing'], c=colors, s=5000*nodes['hf'], cmap='viridis')\n",
    "plt.box(False)\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(ROOT_DIR+\"/models/outputs/figs/ReportESA/synth_score_hf.png\", transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mma = reload(mma)\n",
    "mma.plot_graph(G, name='', figsize=(12,5))\n",
    "plt.savefig(ROOT_DIR+\"/models/outputs/figs/ReportESA/synth_graph.png\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4950/198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot connecting the points\n",
    "plt.plot(fpr, tpr, marker='o', linestyle='dotted')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ramp_to_plateou(pos, slope, start=-np.inf, end= np.inf):\n",
    "    mask =  (pos[:,0] >= start) * (pos[:,0]< end)\n",
    "    ramp = slope*(pos[:,0] - start)\n",
    "    ramp = ramp*mask\n",
    "    ramp[pos[:,0]>=end] = ramp.max()\n",
    "    return ramp\n",
    "\n",
    "np.random.seed(1011)\n",
    "\n",
    "size = 1\n",
    "noise_var = 0.1\n",
    "max_slope = 1\n",
    "pos = G.coords\n",
    "\n",
    "w, V = np.linalg.eigh(G.L.toarray())\n",
    "w[1:] = 0 # Frequency filter\n",
    "\n",
    "displacement = np.random.randn(G.N, size)\n",
    "displacement = V @ np.diag(w) @ V.T @ displacement # filtering\n",
    "# normalizing for desired power\n",
    "displacement = np.sqrt(1*G.N)*displacement/(np.linalg.norm(displacement,axis=0))\n",
    "\n",
    "noise = np.sqrt(noise_var)*np.random.randn(G.N,size)\n",
    "\n",
    "# terrain corresponds to a ramp to a plateou in the horizontal direction\n",
    "slope = max_slope*np.random.rand() # makes small difference given proportional anomaly and scaler\n",
    "start = pos[:,0].max()*np.random.rand()*0.5 # Slope always start in the first half\n",
    "# end = start + (pos[:,0].max()-start)*np.random.rand()\n",
    "\n",
    "min_slope_dist = 100\n",
    "end = start + min_slope_dist + (pos[:,0].max()-start-min_slope_dist)*np.random.rand() #At least 50m of slope\n",
    "\n",
    "terrain = ramp_to_plateou(pos, slope=slope, start=start, end=end).reshape((-1,1))\n",
    "terrain = np.tile(terrain, (1,size)) # Matching number of samples (size)\n",
    "\n",
    "ptp = terrain.ptp()\n",
    "\n",
    "signal = displacement + noise + terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "matplotlib.rcParams.update({'font.family': 'Times New Roman'})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "G.plot_signal(signal, ax=ax, plot_name='')\n",
    "plt.xlabel('West-east')\n",
    "plt.ylabel('North-south')\n",
    "\n",
    "cbar = fig.get_axes()[1]\n",
    "cbar.set_xlabel('test')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "matplotlib.rcParams.update({'font.family': 'Times New Roman'})\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, figsize=(16, 8), gridspec_kw={'height_ratios':[1.5,4]})\n",
    "\n",
    "# ax[0].plot(G.coords[:,0], data[:,snap], linewidth=1)\n",
    "scat = ax[0].scatter(G.coords[:,0], data[:,snap], c=label[:,snap], s=10, cmap= plt.get_cmap('Paired', 2) )\n",
    "# scat = ax[0].scatter(np.arange(198), data[:,snap], c=label[:,snap], s=5, cmap= plt.get_cmap('viridis', 2) )\n",
    "\n",
    "cbar1 = plt.colorbar(scat, ax=ax[0], ticks=[0,1], aspect=7)\n",
    "cbar1.set_label('Fault label')\n",
    "\n",
    "# ax[0].set_xlabel('Node index')\n",
    "ax[0].set_ylabel('Signal [mm]')\n",
    "ax[0].set_xticklabels([])\n",
    "# ax[0].xaxis.set_label_position('top')\n",
    "# ax[0].xaxis.set_ticks_position('top')\n",
    "# ax[0].tick_params(axis='x', which='both', top=True)\n",
    "\n",
    "fault_pos = G.coords[np.where(label[:,snap])[0],:]\n",
    "\n",
    "ax[1].scatter(fault_pos[:,0], fault_pos[:,1], marker='o',linewidth=20, s=1, c='black', linestyle='-')\n",
    "scatter = ax[1].scatter(G.coords[:, 0], G.coords[:, 1], c=signal, cmap='viridis', marker='o', linewidth=5)\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax[1])\n",
    "cbar.set_label('Healthy signal [mm]')\n",
    "\n",
    "# Set labels for the axes\n",
    "ax[1].set_xlabel('Position West-East')\n",
    "ax[1].set_ylabel('Position South-North')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ROOT_DIR+'/models/outputs/figs/ReportESA/synth_signal.png', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = mma.create_data(G, anomaly=0.1, size=25, seed=seed,\n",
    "                                noise_var=1e-5, eigs=1, signal_power=1e-6)\n",
    "plt.plot(data[:,snap])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KarateClub()\n",
    "print(\"Dataset:\", dataset)\n",
    "print(\"# Graphs:\", len(dataset))\n",
    "print(\"# Features:\", dataset.num_features)\n",
    "print(\"# Classes:\", dataset.num_classes)\n",
    "\n",
    "data = dataset[0]\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "nx.draw(G, node_color=data.y, node_size=150)\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GCN, self).__init__()\n",
    "    torch.manual_seed(42)\n",
    "    self.conv1 = GCNConv(dataset.num_features, 4)\n",
    "    self.conv2 = GCNConv(4, 4)\n",
    "    self.conv3 = GCNConv(4, 2)\n",
    "    self.classifier = Linear(2, dataset.num_classes)\n",
    "  def forward(self, x, edge_index):\n",
    "    h = self.conv1(x, edge_index)\n",
    "    h = h.tanh()\n",
    "    h = self.conv2(h, edge_index)\n",
    "    h = h.tanh()\n",
    "    h = self.conv3(h, edge_index)\n",
    "    h = h.tanh()\n",
    "    out = self.classifier(h)\n",
    "    return out, h\n",
    "model = GCN()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "def train(data):\n",
    "  optimizer.zero_grad() # resets the gradient values stored in the parameters\n",
    "  out, h = model(data.x, data.edge_index) # computes forward\n",
    "  loss = criterion(out[data.train_mask], data.y[data.train_mask]) # computes the loss given forward\n",
    "  loss.backward() # computes the gradient given loss and stores in the parameters\n",
    "  optimizer.step() # executes an optimization step using the gradient given backward\n",
    "  return loss, h\n",
    "\n",
    "epochs = range(1, 301) # training iterations\n",
    "losses = []\n",
    "embeddings = []\n",
    "for epoch in epochs: # training process\n",
    "  loss, h = train(data)\n",
    "  losses.append(loss)\n",
    "  embeddings.append(h)\n",
    "  if (not epoch%50) or (epoch==1):\n",
    "    print(f\"Epoch: {epoch}\\tLoss: {loss:.4f}\")\n",
    "\n",
    "out, h = model(data.x, data.edge_index)\n",
    "class_predictions = torch.argmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a PyTorch class\n",
    "# 28*28 ==> 9 ==> 28*28\n",
    "class AE(torch.nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\t\t# Building an linear encoder with Linear\n",
    "\t\t# layer followed by Relu activation function\n",
    "\t\t# 784 ==> 9\n",
    "\t\tself.encoder = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.Linear(28 * 28, 128),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(128, 64),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(64, 36),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(36, 18),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(18, 9)\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Building an linear decoder with Linear\n",
    "\t\t# layer followed by Relu activation function\n",
    "\t\t# The Sigmoid activation function\n",
    "\t\t# outputs the value between 0 and 1\n",
    "\t\t# 9 ==> 784\n",
    "\t\tself.decoder = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.Linear(9, 18),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(18, 36),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(36, 64),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(64, 128),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(128, 28 * 28),\n",
    "\t\t\ttorch.nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tencoded = self.encoder(x)\n",
    "\t\tdecoded = self.decoder(encoded)\n",
    "\t\treturn decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms images to a PyTorch Tensor\n",
    "tensor_transform = transforms.ToTensor()\n",
    "\n",
    "# Download the MNIST Dataset\n",
    "dataset = datasets.MNIST(root = ROOT_DIR + \"/data/raw/\",\n",
    "\t\t\t\t\t\ttrain = True,\n",
    "\t\t\t\t\t\tdownload = False,\n",
    "\t\t\t\t\t\ttransform = tensor_transform)\n",
    "\n",
    "# DataLoader is used to load the dataset\n",
    "# for training\n",
    "loader = torch.utils.data.DataLoader(dataset = dataset,\n",
    "\t\t\t\t\t\t\t\t\tbatch_size = 32,\n",
    "\t\t\t\t\t\t\t\t\tshuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "model = AE()\n",
    "\n",
    "# Validation using MSE Loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "# Using an Adam Optimizer with lr = 0.1\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "\t\t\t\t\t\t\tlr = 1e-4,\n",
    "\t\t\t\t\t\t\tweight_decay = 1e-8)\n",
    "\n",
    "epochs = 50\n",
    "outputs = []\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "\tfor (batch, _) in loader:\n",
    "\t\t\t\n",
    "\t\t# Reshaping the batch to (-1, 784)\n",
    "\t\tbatch = batch.reshape(-1, 28*28)\n",
    "\t\t\n",
    "\t\t# Output of Autoencoder\n",
    "\t\treconstructed = model(batch)\n",
    "\t\t\n",
    "\t\t# Calculating the loss function\n",
    "\t\tloss = loss_function(reconstructed, batch)\n",
    "\t\t\n",
    "\t\t# The gradients are set to zero,\n",
    "\t\t# the gradient is computed and stored.\n",
    "\t\t# .step() performs parameter update\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t\n",
    "\t\t# Storing the losses in a list for plotting\n",
    "\t\tlosses.append(loss)\n",
    "\toutputs.append((epoch, batch, reconstructed))\n",
    "\n",
    "plt.plot([l.item() for l in losses])\n",
    "plt.show()\n",
    "\n",
    "ep = 49\n",
    "for index in range(1):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(4,8))\n",
    "    ax[0].imshow(outputs[ep][1][index,:].reshape(28,28))\n",
    "    ax[1].imshow(outputs[ep][2].detach().numpy()[index,:].reshape(28,28))\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "model = AE()\n",
    "\n",
    "# Validation using MSE Loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "# Using an Adam Optimizer with lr = 0.1\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "\t\t\t\t\t\t\tlr = 5e-3,\n",
    "\t\t\t\t\t\t\tweight_decay = 1e-8)\n",
    "\n",
    "epochs = 45\n",
    "outputs = []\n",
    "losses = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\t\t\t\n",
    "# Reshaping the batch to (-1, 784)\n",
    "    image = dataset.__getitem__(1)[0].reshape(-1,28*28)\n",
    "\n",
    "    # Output of Autoencoder\n",
    "    reconstructed = model(image)\n",
    "\n",
    "    # Calculating the loss function\n",
    "    loss = loss_function(reconstructed, image)\n",
    "\n",
    "    # The gradients are set to zero,\n",
    "    # the gradient is computed and stored.\n",
    "    # .step() performs parameter update\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Storing the losses in a list for plotting\n",
    "    losses.append(loss)\n",
    "    outputs.append((epoch, image, reconstructed))\n",
    "    \n",
    "    # plt.imshow(reconstructed.detach().numpy().reshape(28,28))\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def generate_matrix(epoch):\n",
    "    out = outputs[epoch][2].detach().numpy().reshape(28,28)\n",
    "    inp = outputs[epoch][1].numpy().reshape(28,28)\n",
    "    return np.abs(out-inp)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "def init():\n",
    "    ax.clear()\n",
    "    plt.close()\n",
    "\n",
    "def update(frame):\n",
    "    matrix = generate_matrix(frame)  # Generate the matrix for the current frame\n",
    "    ax.imshow(matrix, cmap='gray', vmin=0, vmax=1)  # Update the plot with the new matrix\n",
    "    # Hide all ticks and tick labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    plt.close()\n",
    "\n",
    "fps = 2\n",
    "ani = FuncAnimation(fig, update, frames=range(epochs), interval=1000/fps, repeat=True, blit=False, init_func=init)\n",
    "# plt.show()\n",
    "\n",
    "plt.plot([l.item() for l in losses])\n",
    "plt.show()\n",
    "\n",
    "HTML(ani.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mma = reload(mma)\n",
    "Nsamples = 10\n",
    "\n",
    "G = mma.synth_graph()\n",
    "edge_index = torch.tensor(np.array(np.nonzero(G.A.toarray())), dtype=torch.long)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_nodes_defect = {10:{'start':0, 'end':5, 'slope':3, 'onset':5},\n",
    "                          13:{'start':0,'end':-5, 'slope':3, 'onset':5},\n",
    "                          187:{'start':0,'end':10, 'slope':5, 'onset':5}}\n",
    "\n",
    "anomalous_nodes_ramping = {node:{\n",
    "                                'start':0,\n",
    "                                'end':10,\n",
    "                                'slope':int(Nsamples//1.5),\n",
    "                                'onset':int(Nsamples//5)\n",
    "                                }\n",
    "                                for node in range(50)}\n",
    "\n",
    "anomalous_nodes_bulge = {node:{\n",
    "                                'start':0,\n",
    "                                'end':np.max((0, 10 - np.abs(node-50)/5)),\n",
    "                                'slope':int(Nsamples//1.5),\n",
    "                                'onset':int(Nsamples//5)\n",
    "                                }\n",
    "                                for node in range(100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hfilter(G, cut=2):\n",
    "    L = G.L.toarray()\n",
    "    w, V = np.linalg.eigh(L)\n",
    "    wh = np.ones(G.N)\n",
    "    wh[w<cut] = 0\n",
    "    Hh = V @ np.diag(wh) @ V.T\n",
    "    return Hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphUNet(1, 3, 1, 3, 0.5)\n",
    "\n",
    "loss_function = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr = 1e-1,\n",
    "                            weight_decay = 1e-18)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "auc = []\n",
    "auc_hf = []\n",
    "for seed in range(20):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    G = mma.synth_graph()\n",
    "    edge_index = torch.tensor(np.array(np.nonzero(G.A.toarray())), dtype=torch.long)\n",
    "\n",
    "    data, label = mma.create_data(G, anomaly=0.1, size=Nsamples, seed=seed,\n",
    "                                noise_var=1e-5, eigs=1, signal_power=1e-6)\n",
    "    # data = scaler.fit_transform(data.reshape(-1,1)).reshape(data.shape)\n",
    "    data = scaler.fit_transform(data)\n",
    "    label_vector = label.reshape((-1,), order='F')\n",
    "\n",
    "    Hh = hfilter(G, cut=2)\n",
    "\n",
    "\n",
    "    error = []\n",
    "    error_hf = []\n",
    "    df_result = []\n",
    "    for snap in range(data.shape[1]):\n",
    "\n",
    "        # GRAPH UNET PART\n",
    "        x = torch.Tensor(data[:,snap]).reshape(-1,1)\n",
    "\n",
    "        model.reset_parameters()\n",
    "\n",
    "        epochs = 75\n",
    "        outputs = []\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "                    \n",
    "            reconstructed = model(x, edge_index)     # Output of Autoencoder\n",
    "            loss = loss_function(reconstructed, x)    # Calculating the loss function\n",
    "            \n",
    "            optimizer.zero_grad() # The gradients are set to zero,\n",
    "            loss.backward() # the gradient is computed and stored.\n",
    "            optimizer.step() # .step() performs parameter update\n",
    "\n",
    "            # Storing the losses in a list for plotting\n",
    "            losses.append(loss)\n",
    "            outputs.append((epoch, x, reconstructed))\n",
    "\n",
    "        error_snap = np.abs(reconstructed.detach() - x).numpy().flatten()\n",
    "        error.extend(error_snap)\n",
    "\n",
    "        # GSP PART\n",
    "        error_hf.extend(np.abs(Hh @ data[:,snap]))\n",
    "        \n",
    "    error = np.array(error).reshape((-1,))\n",
    "    tpr, fpr, thr = roc_params(error, label_vector, interp=True)\n",
    "    auc.append(compute_auc(tpr,fpr))\n",
    "\n",
    "    error_hf = np.array(error_hf).reshape((-1,))\n",
    "    tpr_hf, fpr_hf, thr_hf = roc_params(error_hf, label_vector, interp=True)\n",
    "    auc_hf.append(compute_auc(tpr,fpr))\n",
    "\n",
    "# fig = px.scatter(x=fpr, y=tpr, color=thr,\n",
    "#            labels={'x':'false positive rate', 'y':'true positive rate', 'color':'threshold'},\n",
    "#            width=700, height=500)\n",
    "# fig.update_traces(line={'dash':'dot'}, mode='lines+markers')\n",
    "# fig.show()\n",
    "\n",
    "\n",
    "# mma.relevant_neighborhood(df_result, column_name='error', color='data', lower=0, recenter=[4341000,4479550],\n",
    "#                                     zoom=16.5, figsize=(1000,600), colormap='viridis', transparent=False)\n",
    "\n",
    "# plt.plot([l.item() for l in losses])\n",
    "# plt.title(f'{losses[-1]}')\n",
    "# plt.show()\n",
    "\n",
    "# HTML(graph_anim(outputs,epochs, G).to_jshtml())\n",
    "\n",
    "# model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=fpr, y=[tpr, tpr_hf], color=thr,\n",
    "           labels={'x':'false positive rate', 'y':'true positive rate', 'color':'threshold'},\n",
    "           width=700, height=500)\n",
    "fig.update_traces(line={'dash':'dot'}, mode='lines+markers')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(auc, width=500, height=300).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsamples = 10\n",
    "\n",
    "anomalous_nodes_defect = {10:{'start':0, 'end':5, 'slope':3, 'onset':5},\n",
    "                          13:{'start':0,'end':-5, 'slope':3, 'onset':5},\n",
    "                          187:{'start':0,'end':10, 'slope':5, 'onset':5}}\n",
    "\n",
    "anomalous_nodes_ramping = {node:{\n",
    "                                'start':0,\n",
    "                                'end':10,\n",
    "                                'slope':int(Nsamples//1.5),\n",
    "                                'onset':int(Nsamples//5)\n",
    "                                }\n",
    "                                for node in range(50)}\n",
    "\n",
    "anomalous_nodes_bulge = {node:{\n",
    "                                'start':0,\n",
    "                                'end':np.max((0, 10 - np.abs(node-50)/5)),\n",
    "                                'slope':int(Nsamples//1.5),\n",
    "                                'onset':int(Nsamples//5)\n",
    "                                }\n",
    "                                for node in range(100)}\n",
    "\n",
    "G = mma.synth_graph()\n",
    "edge_index = torch.tensor(np.array(np.nonzero(G.A.toarray())), dtype=torch.long)\n",
    "X = mma.synth_data(anomalous_nodes_defect, G.coords, 10, plot=False)\n",
    "X.sort_values(['timestamp', 'pid'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphUNet(X.timestamp.nunique(), X.timestamp.nunique()*2, X.timestamp.nunique(), 3, 0.5)\n",
    "\n",
    "loss_function = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr = 1e-2,\n",
    "                            weight_decay = 1e-8)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X['data_norm'] = scaler.fit_transform(X[['data']])\n",
    "\n",
    "x = X[['pid','timestamp','data_norm']].pivot(index='pid', columns='timestamp').reset_index(drop=True).values\n",
    "x = torch.Tensor(x)\n",
    "\n",
    "model.reset_parameters()\n",
    "\n",
    "epochs = 50\n",
    "outputs = []\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "            \n",
    "    reconstructed = model(x, edge_index)     # Output of Autoencoder\n",
    "    loss = loss_function(reconstructed, x)    # Calculating the loss function\n",
    "    \n",
    "    optimizer.zero_grad() # The gradients are set to zero,\n",
    "    loss.backward() # the gradient is computed and stored.\n",
    "    optimizer.step() # .step() performs parameter update\n",
    "\n",
    "    # Storing the losses in a list for plotting\n",
    "    losses.append(loss)\n",
    "    outputs.append((epoch, x, reconstructed))\n",
    "\n",
    "out = reconstructed.detach().numpy()\n",
    "inp = x.detach().numpy()\n",
    "\n",
    "df_result = X.drop_duplicates('pid').copy()\n",
    "df_result['mse'] = mean_squared_error(out.T, inp.T, multioutput='raw_values', squared=True)\n",
    "\n",
    "mma.relevant_neighborhood(df_result, column_name='mse', color='data', lower=0, recenter=[4341000,4479550],\n",
    "                                    zoom=16.5, figsize=(1000,600), colormap='viridis', transparent=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a PyTorch class\n",
    "# 28*28 ==> 9 ==> 28*28\n",
    "class AE(torch.nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\t\t# Building an linear encoder with Linear\n",
    "\t\t# layer followed by Relu activation function\n",
    "\t\t# 784 ==> 9\n",
    "\t\tself.encoder = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.Linear(28 * 28, 128),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(128, 64),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(64, 36),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(36, 18),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(18, 9)\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Building an linear decoder with Linear\n",
    "\t\t# layer followed by Relu activation function\n",
    "\t\t# The Sigmoid activation function\n",
    "\t\t# outputs the value between 0 and 1\n",
    "\t\t# 9 ==> 784\n",
    "\t\tself.decoder = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.Linear(9, 18),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(18, 36),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(36, 64),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(64, 128),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(128, 28 * 28),\n",
    "\t\t\ttorch.nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tencoded = self.encoder(x)\n",
    "\t\tdecoded = self.decoder(encoded)\n",
    "\t\treturn decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a PyTorch class\n",
    "# 28*28 ==> 9 ==> 28*28\n",
    "class AE(torch.nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\t\t# Building an linear encoder with Linear\n",
    "\t\t# layer followed by Relu activation function\n",
    "\t\t# 784 ==> 9\n",
    "\t\tself.encoder = Sequential(\n",
    "\t\t\tGCNConv(128, 64),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\tGCNConv(64, 36),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\tGCNConv(36, 18),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\tGCNConv(18, 9)\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Building an linear decoder with Linear\n",
    "\t\t# layer followed by Relu activation function\n",
    "\t\t# The Sigmoid activation function\n",
    "\t\t# outputs the value between 0 and 1\n",
    "\t\t# 9 ==> 784\n",
    "\t\tself.decoder = Sequential(\n",
    "\t\t\tGCNConv(9, 18),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\tGCNConv(18, 36),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\tGCNConv(36, 64),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\tGCNConv(128, 28 * 28),\n",
    "\t\t\ttorch.nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tencoded = self.encoder(x)\n",
    "\t\tdecoded = self.decoder(encoded)\n",
    "\t\treturn decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hfilter(G, cut=2):\n",
    "    L = G.L.toarray()\n",
    "    w, V = np.linalg.eigh(L)\n",
    "    wh = np.ones(G.N)\n",
    "    wh[w<cut] = 0\n",
    "    Hh = V @ np.diag(wh) @ V.T\n",
    "    return Hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = G.L.toarray()\n",
    "w, V = np.linalg.eigh(L)\n",
    "wh = np.ones(G.N)\n",
    "cut = 2\n",
    "wh[w<cut] = 0\n",
    "Hh = V @ np.diag(wh) @ V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mma = reload(mma)\n",
    "\n",
    "auc = []\n",
    "for seed in range(10):\n",
    "    data, label = mma.create_data(G, anomaly=0.1, size=Nsamples, seed=seed,\n",
    "                                noise_var=1e-5, eigs=1, signal_power=1e-6)\n",
    "    # data = scaler.fit_transform(data.reshape(-1,1)).reshape(data.shape)\n",
    "    data = scaler.fit_transform(data)\n",
    "    label_vector = label.reshape((-1,), order='F')\n",
    "\n",
    "    error = []\n",
    "    df_result = []\n",
    "    for snap in range(data.shape[1]):\n",
    "\n",
    "        x = data[:,snap]\n",
    "        y = Hh @ x\n",
    "\n",
    "        error.extend(np.abs(y))\n",
    "        \n",
    "    error = np.array(error).reshape((-1,))\n",
    "    tpr, fpr, thr = roc_params(error, label_vector, interp=True)\n",
    "    auc.append(compute_auc(tpr,fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=fpr, y=tpr, color=thr,\n",
    "           labels={'x':'false positive rate', 'y':'true positive rate', 'color':'threshold'},\n",
    "           width=700, height=500)\n",
    "fig.update_traces(line={'dash':'dot'}, mode='lines+markers')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Hh @ data[:,0])\n",
    "plt.plot(data[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'df_Trondheim_D1L2B'\n",
    "df_orig_full = pd.read_parquet(ROOT_DIR+f\"/data/interim/{dataset}.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mma = reload(mma)\n",
    "\n",
    "radius = 15\n",
    "decay = 2\n",
    "cut = 2\n",
    "\n",
    "df_test = df_orig_full\n",
    "df_metrics = compute_metric(df_orig_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, figsize=(16,12))\n",
    "counts, bins, patches = ax[0].hist(df_metrics.hf, bins=30, log=True, rwidth=0.9, density=True)\n",
    "# ax[0].set_xlabel('Anomaly score')\n",
    "ax[0].set_xlim([0,df_metrics.hf.max()])\n",
    "ax[0].set_title('Density')\n",
    "ax[0].xaxis.set_tick_params(labelsize=12, rotation=-45)\n",
    "ax[0].set_xticks(bins)\n",
    "# ax.yaxis.set_major_formatter(StrMethodf_metricsormatter('{x:.3f}'))\n",
    "# plt.show()\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(16,8))\n",
    "counts, bins, patches = ax[1].hist(df_metrics.hf, bins=30, log=False, rwidth=0.9, cumulative=True, density=True)\n",
    "ax[1].set_xlabel('Anomaly score')\n",
    "ax[1].set_xlim([0,df_metrics.hf.max()])\n",
    "ax[1].set_ylim([0.85,1])\n",
    "ax[1].set_title('Cumulative')\n",
    "ax[1].yaxis.set_major_formatter(StrMethodFormatter('{x:.3f}'))\n",
    "ax[1].xaxis.set_tick_params(labelsize=12, rotation=-90)\n",
    "ax[1].set_xticks(bins)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = 'group'\n",
    "th = 80\n",
    "th_up = np.inf\n",
    "\n",
    "df_relevant, selected = detection(df_metrics, column_name='hf',\n",
    "                                  threshold_min=th, threshold_max=th_up, detection_param_threshold=20)\n",
    "\n",
    "df_var = df_relevant.drop_duplicates('pid').groupby('group').apply(skew).reset_index(name='skewness')\n",
    "df_relevant = df_relevant.merge(df_var, how='left', on='group')\n",
    "df_relevant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics2 = compute_metric(df_orig_full[~df_orig_full.pid.isin(df_relevant.query('group.isin(@selected)').pid.unique())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, figsize=(16,12))\n",
    "counts, bins, patches = ax[0].hist(df_metrics2.hf, bins=30, log=True, rwidth=0.9, density=True)\n",
    "# ax[0].set_xlabel('Anomaly score')\n",
    "ax[0].set_xlim([0,df_metrics2.hf.max()])\n",
    "ax[0].set_title('Density')\n",
    "ax[0].xaxis.set_tick_params(labelsize=12, rotation=-45)\n",
    "ax[0].set_xticks(bins)\n",
    "# ax.yaxis.set_major_formatter(StrMethodf_metrics2ormatter('{x:.3f}'))\n",
    "# plt.show()\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(16,8))\n",
    "counts, bins, patches = ax[1].hist(df_metrics2.hf, bins=30, log=False, rwidth=0.9, cumulative=True, density=True)\n",
    "ax[1].set_xlabel('Anomaly score')\n",
    "ax[1].set_xlim([0,df_metrics2.hf.max()])\n",
    "ax[1].set_ylim([0.85,1])\n",
    "ax[1].set_title('Cumulative')\n",
    "ax[1].yaxis.set_major_formatter(StrMethodFormatter('{x:.3f}'))\n",
    "ax[1].xaxis.set_tick_params(labelsize=12, rotation=-90)\n",
    "ax[1].set_xticks(bins)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = 'group'\n",
    "th = 40\n",
    "th_up = np.inf\n",
    "\n",
    "df_relevant, selected = detection(df_metrics2, column_name='hf',\n",
    "                                  threshold_min=th, threshold_max=th_up, detection_param_threshold=20)\n",
    "\n",
    "df_var = df_relevant.drop_duplicates('pid').groupby('group').apply(skew).reset_index(name='skewness')\n",
    "df_relevant = df_relevant.merge(df_var, how='left', on='group')\n",
    "df_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom = 11.5\n",
    "mma=reload(mma)\n",
    "mma.visualise_mismatch_map(df_relevant,#[df_relevant.group.isin(selected)],\n",
    "                           size='hf',\n",
    "                           range_color=[-50,50], color='smoothed', animation_frame='timestamp',\n",
    "                           title='', transparent=True, hover_data=['group','easting','northing', 'smoothed'],\n",
    "                           figsize=(1200,650), zoom=zoom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mma.relevant_neighborhood(df_metrics2, column_name='hf', lower=40, zoom=11, only_relevant=False, filter_dates=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dario-juiScTYW-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
