{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import scipy as sp\n",
    "# import contextily as cx\n",
    "\n",
    "import torch\n",
    "import pygsp\n",
    "import optuna\n",
    "import joblib\n",
    "import gc\n",
    "import argparse\n",
    "import os\n",
    "import matplotlib\n",
    "import pickle\n",
    "\n",
    "from matplotlib.ticker import ScalarFormatter, StrMethodFormatter, FormatStrFormatter, FuncFormatter\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from optuna.samplers import TPESampler, BruteForceSampler\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn.models import GraphUNet\n",
    "from torch_geometric.nn import GCNConv, Sequential\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx, grid\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from pyprojroot import here\n",
    "ROOT_DIR = str(here())\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "matplotlib.rcParams.update({'font.family': 'Times New Roman'})\n",
    "\n",
    "# Function definitions\n",
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb\n",
    "\n",
    "# SELECT EXPERIMENT TYPE HERE\n",
    "experiment_type = 'random' # 'random' or 'synthetic'\n",
    "\n",
    "def test_model(model, n_epochs, learning_rate, penalty_rate, graph_size):\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    loss_function = torch.nn.MSELoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                lr = learning_rate,\n",
    "                                weight_decay = penalty_rate)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    auc = []\n",
    "\n",
    "    test_seed_offset = 1001  #offset for picking different seeds from hyperparameter tuning\n",
    "\n",
    "    for test_seed in range(50):\n",
    "\n",
    "        seed = test_seed_offset + test_seed\n",
    "\n",
    "        print(f'seed:{seed}')\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Random graph\n",
    "        G = pygsp.graphs.Sensor(graph_size, seed=seed, n_try=200)\n",
    "        G.coords = G.coords*100\n",
    "\n",
    "        # # Synthetic graph\n",
    "        # G = generate_synthetic_graph(N=graph_size)\n",
    "\n",
    "        edge_index = torch.tensor(np.array(np.nonzero(G.A.toarray())), dtype=torch.long)\n",
    "        data, label = generate_data(G, size=20, anomaly_type=2)\n",
    "\n",
    "        data = scaler.fit_transform(data)\n",
    "        label_vector = label.reshape((-1,), order='F')\n",
    "\n",
    "        error = []\n",
    "        \n",
    "        for snap in range(data.shape[1]):\n",
    "\n",
    "            x = torch.Tensor(data[:,snap]).reshape(-1,1)\n",
    "\n",
    "            model.reset_parameters()\n",
    "\n",
    "            epochs = n_epochs\n",
    "            outputs = []\n",
    "            losses = []\n",
    "            for epoch in range(epochs):\n",
    "                        \n",
    "                reconstructed = model(x, edge_index)     # Output of Autoencoder\n",
    "                loss = loss_function(reconstructed, x)    # Calculating the loss function\n",
    "                \n",
    "                optimizer.zero_grad() # The gradients are set to zero,\n",
    "                loss.backward() # the gradient is computed and stored.\n",
    "                optimizer.step() # .step() performs parameter update\n",
    "\n",
    "                # Storing the losses in a list for plotting\n",
    "                losses.append(loss)\n",
    "                outputs.append((epoch, x, reconstructed))\n",
    "\n",
    "            error_snap = np.abs(reconstructed.detach() - x).numpy().flatten()\n",
    "            error.extend(error_snap)\n",
    "            \n",
    "        error = np.array(error).reshape((-1,))\n",
    "        tpr, fpr, thr = roc_params(error, label_vector, interp=True)\n",
    "        auc.append(compute_auc(tpr,fpr))\n",
    "\n",
    "    return auc\n",
    "\n",
    "\n",
    "auc_list = []\n",
    "for graph_size in [5, 10, 25, 50, 100, 150, 200]:\n",
    "    print(f'Graph size:{graph_size}')\n",
    "\n",
    "    best_params = joblib.load(f\"../outputs/optuna_gunet_{experiment_type}/optimization_logs_{graph_size}.pkl\").best_params\n",
    "\n",
    "    n_epochs = best_params['n_epochs']\n",
    "    learning_rate = best_params['learning_rate']\n",
    "    penalty_rate = best_params['penalty_rate']\n",
    "    hidden_channels = best_params['hidden_channels']\n",
    "    depth = best_params['depth']\n",
    "    pool_ratios = best_params['pool_ratios']\n",
    "\n",
    "    model = GraphUNet(1, hidden_channels, 1, depth, pool_ratios)\n",
    "\n",
    "    auc_list.append(test_model(model, n_epochs, learning_rate, penalty_rate, graph_size))\n",
    "    \n",
    "\n",
    "log_dir = ROOT_DIR + '/outputs/testing/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir,exist_ok=True)\n",
    "\n",
    "log_file = log_dir + f'auc_results_gunet_{experiment_type}.pkl'\n",
    "joblib.dump(auc_list, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted spectral energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb\n",
    "\n",
    "# SELECT EXPERIMENT TYPE HERE\n",
    "experiment_type = 'random' # 'random' or 'synthetic'\n",
    "\n",
    "def test_model(cut, decay, graph_size):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    auc = []\n",
    "\n",
    "    test_seed_offset = 1001  #offset for picking different seeds from hyperparameter tuning\n",
    "\n",
    "    for test_seed in range(50):\n",
    "\n",
    "        seed = test_seed_offset + test_seed\n",
    "\n",
    "        print(f'seed:{seed}')\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # # Random graph\n",
    "        # G = pygsp.graphs.Sensor(graph_size, seed=seed, n_try=200)\n",
    "        # G.coords = G.coords*100\n",
    "\n",
    "        # Synthetic graph\n",
    "        G = generate_synthetic_graph(N=graph_size)\n",
    "        \n",
    "        \n",
    "        nodes = pd.DataFrame({'easting':G.coords[:,0], 'northing':G.coords[:,1]})\n",
    "\n",
    "        data, label = generate_data(G, size=20, anomaly_type=2)\n",
    "        data = scaler.fit_transform(data)\n",
    "\n",
    "        label_vector = label.reshape((-1,), order='F')\n",
    "\n",
    "        error = []\n",
    "        \n",
    "        for snap in range(data.shape[1]):\n",
    "\n",
    "            wse = compute_wse(nodes, x=data[:,snap].reshape((-1,1)), decay=decay, cut=cut)\n",
    "\n",
    "            error.extend(wse)\n",
    "            \n",
    "        error = np.array(error).reshape((-1,))\n",
    "        tpr, fpr, thr = roc_params(error, label_vector, interp=True)\n",
    "        auc.append(compute_auc(tpr,fpr))\n",
    "\n",
    "    return auc\n",
    "\n",
    "\n",
    "auc_list = []\n",
    "for graph_size in [5, 10, 25, 50, 100, 150, 200]:\n",
    "    print(f'Graph size:{graph_size}')\n",
    "\n",
    "    best_params = joblib.load(f\"../outputs/optuna_wse_{experiment_type}/optimization_logs_{graph_size}.pkl\").best_params\n",
    "\n",
    "    cut = best_params['cut']\n",
    "    decay = best_params['decay']\n",
    "\n",
    "    auc_list.append(test_model(cut, decay, graph_size))\n",
    "    \n",
    "\n",
    "log_dir = ROOT_DIR + '/outputs/testing/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir,exist_ok=True)\n",
    "\n",
    "log_file = log_dir + f'auc_results_wse_{experiment_type}.pkl'\n",
    "joblib.dump(auc_list, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb\n",
    "\n",
    "# SELECT EXPERIMENT TYPE HERE\n",
    "experiment_type = 'random' # 'random' or 'synthetic'\n",
    "\n",
    "def test_model(cut, graph_size):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    auc = []\n",
    "\n",
    "    test_seed_offset = 1001  #offset for picking different seeds from hyperparameter tuning\n",
    "\n",
    "    for test_seed in range(50):\n",
    "\n",
    "        seed = test_seed_offset + test_seed\n",
    "\n",
    "        print(f'seed:{seed}')\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Random graph\n",
    "        G = pygsp.graphs.Sensor(graph_size, seed=seed, n_try=200)\n",
    "        G.coords = G.coords*100\n",
    "\n",
    "        # # Synthetic graph\n",
    "        # G = generate_synthetic_graph(N=graph_size)\n",
    "        \n",
    "        data, label = generate_data(G, size=20, anomaly_type=2)\n",
    "        data = scaler.fit_transform(data)\n",
    "\n",
    "        label_vector = label.reshape((-1,), order='F')\n",
    "\n",
    "        Hh = hfilter(G, cut)\n",
    "\n",
    "        error = []\n",
    "        \n",
    "        for snap in range(data.shape[1]):\n",
    "\n",
    "            error.extend(np.abs(Hh @ data[:,snap]))\n",
    "            \n",
    "        error = np.array(error).reshape((-1,))\n",
    "        tpr, fpr, thr = roc_params(error, label_vector, interp=True)\n",
    "        auc.append(compute_auc(tpr,fpr))\n",
    "\n",
    "    return auc\n",
    "\n",
    "\n",
    "auc_list = []\n",
    "for graph_size in [5, 10, 25, 50, 100, 150, 200]:\n",
    "    print(f'Graph size:{graph_size}')\n",
    "\n",
    "    best_params = joblib.load(f\"../outputs/optuna_gfilter_{experiment_type}/optimization_logs_{graph_size}.pkl\").best_params\n",
    "\n",
    "    cut = best_params['cut']\n",
    "\n",
    "    auc_list.append(test_model(cut, graph_size))\n",
    "    \n",
    "\n",
    "log_dir = ROOT_DIR + '/outputs/testing/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir,exist_ok=True)\n",
    "\n",
    "log_file = log_dir + f'auc_results_gfilter_{experiment_type}.pkl'\n",
    "joblib.dump(auc_list, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['gunet_random', 'wse_random', 'gfilter_random', 'gunet_synthetic', 'wse_synthetic','gfilter_synthetic']\n",
    "\n",
    "for method in methods:\n",
    "    auc_list = joblib.load(f'../outputs/testing/auc_results_{method}.pkl')\n",
    "    print(np.mean(auc_list,axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igarss-REga8jWa-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
